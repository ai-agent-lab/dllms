\begin{algorithm}[H]
\footnotesize
\caption{Adaptation Training (Reproduced from \cite{gong_scaling_2025})}
\label{alg:training}
\begin{algorithmic}[1]
\State \textbf{Input:} network $f_{\theta}$ initialized by existing models, training corpus $p_{data}(\bm{x}_{0}^{1:N})$, mask token $\bm{m}$
\State \textbf{Output:} model parameters $\theta$
\Repeat
    \State Draw $\bm{x}_{0}^{1:N}\sim p_{data}$ and set \textit{labels} $\gets \bm{x}_{0}^{1:N}$
    \State Sample $t \sim \text{Uniform}(0,1)$
    \State Sample $\bm{x}_{t}^{1:N} \sim q(\bm{x}_{t}|\bm{x}_{0})$
    \State Anneal the attention mask \texttt{attn\_mask}
    \State Forward pass: \textit{logits} $\gets f_{\theta}(\bm{x}_{t}^{1:N})$ with \texttt{attn\_mask}
    \State Right shift \textit{logits} by one position \Comment{see Eq.~\ref{eq:dm-loss}}
    \State $\mathcal{L}_t = \frac{1}{t} \delta_{x_t, m} \cdot \text{CE}(\textit{logits}, \textit{labels})$
    \State Backpropagate using $\mathcal{L}_t$ and update $\theta$
\Until{convergence}
\end{algorithmic}
\end{algorithm}
\begin{algorithm}[t]
\footnotesize
\caption{diffu-GRPO: Policy Gradient Optimization for Masked dLLMs (Reproduced from d1~\cite{zhao_d1_2025})}
\label{alg:diffu-grpo}
\begin{algorithmic}[1]
\State \textbf{Require:} Reference model $\pi_{\text{ref}}$, prompt distribution $\mathcal{D}$, number of completions per prompt $G$, number of inner updates $\mu$, prompt token masking probability $p_{\text{mask}}$
\State \textbf{Initialize} $\pi_\theta \leftarrow \pi_{\text{ref}}$
\While{not converged}
    \State $\pi_{\text{old}} \leftarrow \pi_\theta$
    \State Sample a prompt $q \sim \mathcal{D}$
    \State Sample $G$ completions $o_i \sim \pi_{\theta_{\text{old}}}(\cdot | q)$, $i \in [G]$\
    \State For each $o_i$, compute reward $r_i$ and advantage $A_i^k(\pi_{\theta_{\text{old}}})$ using Eq.~\ref{eq:grpo-advantage}
    \For{gradient update iterations $n = 1, \ldots, \mu$}
        \State $q' \leftarrow$ randomly mask tokens of prompt $p$ with probability $p_{\text{mask}}$
        \State For $\pi_\theta, \pi_{\theta_{\text{old}}}, \pi_{\text{ref}}$, estimate log-probabilities of $o_i$ given $q'$ 
        \State Compute diffu-GRPO objective (\ref{eq:diffugrpo}) and update $\pi_\theta$ by gradient descent
    \EndFor
\EndWhile
\State \Return $\pi_\theta$
\end{algorithmic}
\end{algorithm}
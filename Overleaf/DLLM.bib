@article{yi_diffusion_2024,
	title = {Diffusion models in text generation: a survey},
	volume = {10},
	issn = {2376-5992},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10909201/},
	doi = {10.7717/peerj-cs.1905},
	shorttitle = {Diffusion models in text generation},
	abstract = {Diffusion models are a kind of math-based model that were first applied to image generation. Recently, they have drawn wide interest in natural language generation ({NLG}), a sub-field of natural language processing ({NLP}), due to their capability to generate varied and high-quality text outputs. In this article, we conduct a comprehensive survey on the application of diffusion models in text generation. We divide text generation into three parts (conditional, unconstrained, and multi-mode text generation, respectively) and provide a detailed introduction. In addition, considering that autoregressive-based pre-training models ({PLMs}) have recently dominated text generation, we conduct a detailed comparison between diffusion models and {PLMs} in multiple dimensions, highlighting their respective advantages and limitations. We believe that integrating {PLMs} into diffusion is a valuable research avenue. We also discuss current challenges faced by diffusion models in text generation and propose potential future research directions, such as improving sampling speed to address scalability issues and exploring multi-modal text generation. By providing a comprehensive analysis and outlook, this survey will serve as a valuable reference for researchers and practitioners interested in utilizing diffusion models for text generation tasks.},
	pages = {e1905},
	journaltitle = {{PeerJ} Computer Science},
	shortjournal = {{PeerJ} Comput Sci},
	author = {Yi, Qiuhua and Chen, Xiangfan and Zhang, Chenwei and Zhou, Zehai and Zhu, Linan and Kong, Xiangjie},
	urldate = {2025-03-20},
	date = {2024-02-23},
	pmid = {38435628},
	pmcid = {PMC10909201},
	keywords = {Machine Learning, Literature Review, Computer Science - Machine Learning, Computer Science - Computation and Language, {CT}-read, Interporablity},
	file = {PubMed Central Full Text PDF:/Users/moonshine/Zotero/storage/N6DKZD6E/Yi et al. - 2024 - Diffusion models in text generation a survey.pdf:application/pdf},
}

@misc{rutte_generalized_2025,
	title = {Generalized Interpolating Discrete Diffusion},
	url = {http://arxiv.org/abs/2503.04482},
	doi = {10.48550/arXiv.2503.04482},
	abstract = {While state-of-the-art language models achieve impressive results through next-token prediction, they have inherent limitations such as the inability to revise already generated tokens. This has prompted exploration of alternative approaches such as discrete diffusion. However, masked diffusion, which has emerged as a popular choice due to its simplicity and effectiveness, reintroduces this inability to revise words. To overcome this, we generalize masked diffusion and derive the theoretical backbone of a family of general interpolating discrete diffusion ({GIDD}) processes offering greater flexibility in the design of the noising processes. Leveraging a novel diffusion {ELBO}, we achieve compute-matched state-of-the-art performance in diffusion language modeling. Exploiting {GIDD}'s flexibility, we explore a hybrid approach combining masking and uniform noise, leading to improved sample quality and unlocking the ability for the model to correct its own mistakes, an area where autoregressive models notoriously have struggled. Our code and models are open-source: https://github.com/dvruette/gidd/},
	number = {{arXiv}:2503.04482},
	publisher = {{arXiv}},
	author = {Rütte, Dimitri von and Fluri, Janis and Ding, Yuhui and Orvieto, Antonio and Schölkopf, Bernhard and Hofmann, Thomas},
	urldate = {2025-04-18},
	date = {2025-03-06},
	eprinttype = {arxiv},
	eprint = {2503.04482 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, {CT}-read, Interporablity, Theory},
	file = {2503.04482v1.pdf:/Users/moonshine/Zotero/storage/U9TU9RIP/2503.04482v1.pdf:application/pdf;Snapshot:/Users/moonshine/Zotero/storage/6XTW6IZE/2503.html:text/html},
}

@misc{gong_scaling_2025,
	title = {Scaling Diffusion Language Models via Adaptation from Autoregressive Models},
	url = {http://arxiv.org/abs/2410.17891},
	doi = {10.48550/arXiv.2410.17891},
	abstract = {Diffusion Language Models ({DLMs}) have emerged as a promising new paradigm for text generative modeling, potentially addressing limitations of autoregressive ({AR}) models. However, current {DLMs} have been studied at a smaller scale compared to their {AR} counterparts and lack fair comparison on language modeling benchmarks. Additionally, training diffusion models from scratch at scale remains challenging. Given the prevalence of open-source {AR} language models, we propose adapting these models to build text diffusion models. We demonstrate connections between {AR} and diffusion modeling objectives and introduce a simple continual pre-training approach for training diffusion models. Through systematic evaluation on language modeling, reasoning, and commonsense benchmarks, we show that we can convert {AR} models ranging from 127M to 7B parameters ({GPT}2 and {LLaMA}) into diffusion models {DiffuGPT} and {DiffuLLaMA}, using less than 200B tokens for training. Our experimental results reveal that these models outperform earlier {DLMs} and are competitive with their {AR} counterparts. We release a suite of {DLMs} (127M-355M-7B) capable of generating fluent text, performing in-context learning, filling in the middle without prompt re-ordering, and following instructions https://github.com/{HKUNLP}/{DiffuLLaMA}.},
	number = {{arXiv}:2410.17891},
	publisher = {{arXiv}},
	author = {Gong, Shansan and Agarwal, Shivam and Zhang, Yizhe and Ye, Jiacheng and Zheng, Lin and Li, Mukai and An, Chenxin and Zhao, Peilin and Bi, Wei and Han, Jiawei and Peng, Hao and Kong, Lingpeng},
	urldate = {2025-04-15},
	date = {2025-02-24},
	eprinttype = {arxiv},
	eprint = {2410.17891 [cs]},
	keywords = {Computer Science - Computation and Language, Value, Interporablity, {InferenceSpeed}, {ICLR}2025, Theory},
	file = {Full Text PDF:/Users/moonshine/Zotero/storage/BDBTY45N/Gong et al. - 2025 - Scaling Diffusion Language Models via Adaptation from Autoregressive Models.pdf:application/pdf;Snapshot:/Users/moonshine/Zotero/storage/NBC5SYB9/2410.html:text/html},
}

@misc{ye_diffusion_2024,
	title = {Diffusion of Thoughts: Chain-of-Thought Reasoning in Diffusion Language Models},
	url = {http://arxiv.org/abs/2402.07754},
	doi = {10.48550/arXiv.2402.07754},
	shorttitle = {Diffusion of Thoughts},
	abstract = {Recently, diffusion models have garnered significant interest in the field of text processing due to their many potential advantages compared to conventional autoregressive models. In this work, we propose Diffusion-of-Thought ({DoT}), a novel approach that integrates diffusion models with Chain-of-Thought, a well-established technique for improving the reasoning ability of autoregressive language models. In contrast to autoregressive language models that make decisions in a left-to-right, token-by-token manner, {DoT} allows reasoning steps to diffuse over time through a diffusion language model and offers greater flexibility in trading-off computation for reasoning performance. Our experimental results demonstrate the effectiveness of {DoT} in multi-digit multiplication, boolean logic, and grade school math problems, with a small diffusion model outperforming a much larger autoregressive model in both efficiency and accuracy. In addition to that, {DoT} showcases promising self-correction abilities and benefits from existing reasoning-enhancing techniques like self-consistency decoding. Our findings contribute to the understanding and development of reasoning with diffusion language models.},
	number = {{arXiv}:2402.07754},
	publisher = {{arXiv}},
	author = {Ye, Jiacheng and Gong, Shansan and Chen, Liheng and Zheng, Lin and Gao, Jiahui and Shi, Han and Wu, Chuan and Jiang, Xin and Li, Zhenguo and Bi, Wei and Kong, Lingpeng},
	urldate = {2025-04-14},
	date = {2024-12-05},
	eprinttype = {arxiv},
	eprint = {2402.07754 [cs]},
	keywords = {Machine Learning, Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, {CT}-read, Theory},
	file = {Full Text PDF:/Users/moonshine/Zotero/storage/7V4RDBYD/Ye et al. - 2024 - Diffusion of Thoughts Chain-of-Thought Reasoning in Diffusion Language Models.pdf:application/pdf;Snapshot:/Users/moonshine/Zotero/storage/SZITI7IY/2402.html:text/html},
}

@misc{nie_large_2025,
	title = {Large Language Diffusion Models},
	url = {http://arxiv.org/abs/2502.09992},
	doi = {10.48550/arXiv.2502.09992},
	abstract = {Autoregressive models ({ARMs}) are widely regarded as the cornerstone of large language models ({LLMs}). We challenge this notion by introducing {LLaDA}, a diffusion model trained from scratch under the pre-training and supervised fine-tuning ({SFT}) paradigm. {LLaDA} models distributions through a forward data masking process and a reverse process, parameterized by a vanilla Transformer to predict masked tokens. By optimizing a likelihood bound, it provides a principled generative approach for probabilistic inference. Across extensive benchmarks, {LLaDA} demonstrates strong scalability, outperforming our self-constructed {ARM} baselines. Remarkably, {LLaDA} 8B is competitive with strong {LLMs} like {LLaMA}3 8B in in-context learning and, after {SFT}, exhibits impressive instruction-following abilities in case studies such as multi-turn dialogue. Moreover, {LLaDA} addresses the reversal curse, surpassing {GPT}-4o in a reversal poem completion task. Our findings establish diffusion models as a viable and promising alternative to {ARMs}, challenging the assumption that key {LLM} capabilities discussed above are inherently tied to {ARMs}. Project page and codes: https://ml-gsai.github.io/{LLaDA}-demo/.},
	number = {{arXiv}:2502.09992},
	publisher = {{arXiv}},
	author = {Nie, Shen and Zhu, Fengqi and You, Zebin and Zhang, Xiaolu and Ou, Jingyang and Hu, Jun and Zhou, Jun and Lin, Yankai and Wen, Ji-Rong and Li, Chongxuan},
	urldate = {2025-03-21},
	date = {2025-02-18},
	eprinttype = {arxiv},
	eprint = {2502.09992 [cs]},
	keywords = {Machine Learning, Computer Science - Machine Learning, Computer Science - Computation and Language, {CT}-read},
	file = {2502.09992v2.pdf:/Users/moonshine/Zotero/storage/9H5DM7R7/2502.09992v2.pdf:application/pdf;Snapshot:/Users/moonshine/Zotero/storage/U8Y48QJ9/2502.html:text/html},
}

@misc{gong_diffuseq_2023,
	title = {{DiffuSeq}: Sequence to Sequence Text Generation with Diffusion Models},
	url = {http://arxiv.org/abs/2210.08933},
	doi = {10.48550/arXiv.2210.08933},
	shorttitle = {{DiffuSeq}},
	abstract = {Recently, diffusion models have emerged as a new paradigm for generative models. Despite the success in domains using continuous signals such as vision and audio, adapting diffusion models to natural language is under-explored due to the discrete nature of texts, especially for conditional generation. We tackle this challenge by proposing {DiffuSeq}: a diffusion model designed for sequence-to-sequence (Seq2Seq) text generation tasks. Upon extensive evaluation over a wide range of Seq2Seq tasks, we find {DiffuSeq} achieving comparable or even better performance than six established baselines, including a state-of-the-art model that is based on pre-trained language models. Apart from quality, an intriguing property of {DiffuSeq} is its high diversity during generation, which is desired in many Seq2Seq tasks. We further include a theoretical analysis revealing the connection between {DiffuSeq} and autoregressive/non-autoregressive models. Bringing together theoretical analysis and empirical evidence, we demonstrate the great potential of diffusion models in complex conditional language generation tasks. Code is available at {\textbackslash}url\{https://github.com/Shark-{NLP}/{DiffuSeq}\}},
	number = {{arXiv}:2210.08933},
	publisher = {{arXiv}},
	author = {Gong, Shansan and Li, Mukai and Feng, Jiangtao and Wu, Zhiyong and Kong, Lingpeng},
	urldate = {2025-04-23},
	date = {2023-02-14},
	eprinttype = {arxiv},
	eprint = {2210.08933 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, {CT}-read, Interporablity, Application, Theory},
	file = {Preprint PDF:/Users/moonshine/Zotero/storage/LBZ58YKQ/Gong et al. - 2023 - DiffuSeq Sequence to Sequence Text Generation with Diffusion Models.pdf:application/pdf;Snapshot:/Users/moonshine/Zotero/storage/6HCSVKSK/2210.html:text/html},
}

@misc{koh_conditional_2025,
	title = {Conditional [{MASK}] Discrete Diffusion Language Model},
	url = {http://arxiv.org/abs/2411.06438},
	doi = {10.48550/arXiv.2411.06438},
	abstract = {Although auto-regressive models excel in natural language processing, they often struggle to generate diverse text and provide limited controllability. Non-auto-regressive methods could be an alternative but often produce degenerate outputs and exhibit shortcomings in conditional generation. To address these challenges, we propose Diffusion-{EAGS}, a novel framework that integrates conditional masked language models into diffusion language models through the theoretical lens of a conditional Markov Random Field. In doing so, we propose entropy-adaptive Gibbs sampling and entropy-based noise scheduling to counterbalance each model's shortcomings. Experimental results show that Diffusion-{EAGS} outperforms baselines and achieves the best quality-diversity tradeoff, demonstrating its effectiveness in non-autoregressive text generation.},
	number = {{arXiv}:2411.06438},
	publisher = {{arXiv}},
	author = {Koh, Hyukhun and Jhang, Minha and Kim, Dohyung and Lee, Sangmook and Jung, Kyomin},
	urldate = {2025-04-23},
	date = {2025-02-24},
	eprinttype = {arxiv},
	eprint = {2411.06438 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, {CT}-read, Application},
	file = {Preprint PDF:/Users/moonshine/Zotero/storage/PNP3XVT9/Koh et al. - 2025 - Conditional [MASK] Discrete Diffusion Language Model.pdf:application/pdf;Snapshot:/Users/moonshine/Zotero/storage/KF2KHYYF/2411.html:text/html},
}

@misc{chen_cheaper_2023,
	title = {A Cheaper and Better Diffusion Language Model with Soft-Masked Noise},
	url = {http://arxiv.org/abs/2304.04746},
	doi = {10.48550/arXiv.2304.04746},
	abstract = {Diffusion models that are based on iterative denoising have been recently proposed and leveraged in various generation tasks like image generation. Whereas, as a way inherently built for continuous data, existing diffusion models still have some limitations in modeling discrete data, e.g., languages. For example, the generally used Gaussian noise can not handle the discrete corruption well, and the objectives in continuous spaces fail to be stable for textual data in the diffusion process especially when the dimension is high. To alleviate these issues, we introduce a novel diffusion model for language modeling, Masked-Diffuse {LM}, with lower training cost and better performances, inspired by linguistic features in languages. Specifically, we design a linguistic-informed forward process which adds corruptions to the text through strategically soft-masking to better noise the textual data. Also, we directly predict the categorical distribution with cross-entropy loss function in every diffusion step to connect the continuous space and discrete space in a more efficient and straightforward way. Through experiments on 5 controlled generation tasks, we demonstrate that our Masked-Diffuse {LM} can achieve better generation quality than the state-of-the-art diffusion models with better efficiency.},
	number = {{arXiv}:2304.04746},
	publisher = {{arXiv}},
	author = {Chen, Jiaao and Zhang, Aston and Li, Mu and Smola, Alex and Yang, Diyi},
	urldate = {2025-04-23},
	date = {2023-04-10},
	eprinttype = {arxiv},
	eprint = {2304.04746 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, {CT}-read, {InferenceSpeed}, Application},
	file = {Preprint PDF:/Users/moonshine/Zotero/storage/7S7LJ3FB/Chen et al. - 2023 - A Cheaper and Better Diffusion Language Model with Soft-Masked Noise.pdf:application/pdf;Snapshot:/Users/moonshine/Zotero/storage/3NET69FX/2304.html:text/html},
}

@misc{lou_discrete_2024,
	title = {Discrete Diffusion Modeling by Estimating the Ratios of the Data Distribution},
	url = {http://arxiv.org/abs/2310.16834},
	doi = {10.48550/arXiv.2310.16834},
	abstract = {Despite their groundbreaking performance for many generative modeling tasks, diffusion models have fallen short on discrete data domains such as natural language. Crucially, standard diffusion models rely on the well-established theory of score matching, but efforts to generalize this to discrete structures have not yielded the same empirical gains. In this work, we bridge this gap by proposing score entropy, a novel loss that naturally extends score matching to discrete spaces, integrates seamlessly to build discrete diffusion models, and significantly boosts performance. Experimentally, we test our Score Entropy Discrete Diffusion models ({SEDD}) on standard language modeling tasks. For comparable model sizes, {SEDD} beats existing language diffusion paradigms (reducing perplexity by \$25\$-\$75\${\textbackslash}\%) and is competitive with autoregressive models, in particular outperforming {GPT}-2. Furthermore, compared to autoregressive mdoels, {SEDD} generates faithful text without requiring distribution annealing techniques like temperature scaling (around \$6\$-\$8{\textbackslash}times\$ better generative perplexity than un-annealed {GPT}-2), can trade compute and quality (similar quality with \$32{\textbackslash}times\$ fewer network evaluations), and enables controllable infilling (matching nucleus sampling quality while enabling other strategies besides left to right prompting).},
	number = {{arXiv}:2310.16834},
	publisher = {{arXiv}},
	author = {Lou, Aaron and Meng, Chenlin and Ermon, Stefano},
	urldate = {2025-04-23},
	date = {2024-06-06},
	eprinttype = {arxiv},
	eprint = {2310.16834 [stat]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, Value, Statistics - Machine Learning, {CT}-read, {InferenceSpeed}, Theory},
	file = {Preprint PDF:/Users/moonshine/Zotero/storage/SK5K3Q7X/Lou et al. - 2024 - Discrete Diffusion Modeling by Estimating the Ratios of the Data Distribution.pdf:application/pdf;Snapshot:/Users/moonshine/Zotero/storage/KNLYWLES/2310.html:text/html},
}

@misc{gulrajani_likelihood-based_2023,
	title = {Likelihood-Based Diffusion Language Models},
	url = {http://arxiv.org/abs/2305.18619},
	doi = {10.48550/arXiv.2305.18619},
	abstract = {Despite a growing interest in diffusion-based language models, existing work has not shown that these models can attain nontrivial likelihoods on standard language modeling benchmarks. In this work, we take the first steps towards closing the likelihood gap between autoregressive and diffusion-based language models, with the goal of building and releasing a diffusion model which outperforms a small but widely-known autoregressive model. We pursue this goal through algorithmic improvements, scaling laws, and increased compute. On the algorithmic front, we introduce several methodological improvements for the maximum-likelihood training of diffusion language models. We then study scaling laws for our diffusion models and find compute-optimal training regimes which differ substantially from autoregressive models. Using our methods and scaling analysis, we train and release Plaid 1B, a large diffusion language model which outperforms {GPT}-2 124M in likelihood on benchmark datasets and generates fluent samples in unconditional and zero-shot control settings.},
	number = {{arXiv}:2305.18619},
	publisher = {{arXiv}},
	author = {Gulrajani, Ishaan and Hashimoto, Tatsunori B.},
	urldate = {2025-04-23},
	date = {2023-05-30},
	eprinttype = {arxiv},
	eprint = {2305.18619 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, Application},
	file = {Preprint PDF:/Users/moonshine/Zotero/storage/A258KTEY/Gulrajani and Hashimoto - 2023 - Likelihood-Based Diffusion Language Models.pdf:application/pdf;Snapshot:/Users/moonshine/Zotero/storage/8H2X75SA/2305.html:text/html},
}

@misc{yin_best_2025,
	title = {The Best of Both Worlds: Integrating Language Models and Diffusion Models for Video Generation},
	url = {http://arxiv.org/abs/2503.04606},
	doi = {10.48550/arXiv.2503.04606},
	shorttitle = {The Best of Both Worlds},
	abstract = {Recent advancements in text-to-video (T2V) generation have been driven by two competing paradigms: autoregressive language models and diffusion models. However, each paradigm has intrinsic limitations: language models struggle with visual quality and error accumulation, while diffusion models lack semantic understanding and causal modeling. In this work, we propose {LanDiff}, a hybrid framework that synergizes the strengths of both paradigms through coarse-tofine generation. Our architecture introduces three key innovations: (1) a semantic tokenizer that compresses 3D visual features into compact 1D discrete representations through efficient semantic compression, achieving a ∼14,000× compression ratio; (2) a language model that generates semantic tokens with high-level semantic relationships; (3) a streaming diffusion model that refines coarse semantics into high-fidelity videos. Experiments show that {LanDiff}, a 5B model, achieves a score of 85.43 on the {VBench} T2V benchmark, surpassing the state-of-the-art opensource models Hunyuan Video (13B) and other commercial models such as Sora, Keling, and Hailuo. Furthermore, our model also achieves state-of-the-art performance in long video generation, surpassing other open-source models in this field. Our demo can be viewed at https: //landiff.github.io/.},
	number = {{arXiv}:2503.04606},
	publisher = {{arXiv}},
	author = {Yin, Aoxiong and Shen, Kai and Leng, Yichong and Tan, Xu and Zhou, Xinyu and Li, Juncheng and Tang, Siliang},
	urldate = {2025-04-23},
	date = {2025-03-08},
	langid = {english},
	eprinttype = {arxiv},
	eprint = {2503.04606 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Computation and Language, {CT}-read, Eric-read},
	file = {PDF:/Users/moonshine/Zotero/storage/5T4KUFGL/Yin et al. - 2025 - The Best of Both Worlds Integrating Language Models and Diffusion Models for Video Generation.pdf:application/pdf},
}

@misc{lovelace_latent_2023,
	title = {Latent Diffusion for Language Generation},
	url = {http://arxiv.org/abs/2212.09462},
	doi = {10.48550/arXiv.2212.09462},
	abstract = {Diffusion models have achieved great success in modeling continuous data modalities such as images, audio, and video, but have seen limited use in discrete domains such as language. Recent attempts to adapt diffusion to language have presented diffusion as an alternative to existing pretrained language models. We view diffusion and existing language models as complementary. We demonstrate that encoder-decoder language models can be utilized to efficiently learn high-quality language autoencoders. We then demonstrate that continuous diffusion models can be learned in the latent space of the language autoencoder, enabling us to sample continuous latent representations that can be decoded into natural language with the pretrained decoder. We validate the effectiveness of our approach for unconditional, class-conditional, and sequence-to-sequence language generation. We demonstrate across multiple diverse data sets that our latent language diffusion models are significantly more effective than previous diffusion language models.},
	number = {{arXiv}:2212.09462},
	publisher = {{arXiv}},
	author = {Lovelace, Justin and Kishore, Varsha and Wan, Chao and Shekhtman, Eliot and Weinberger, Kilian Q.},
	urldate = {2025-04-23},
	date = {2023-11-07},
	eprinttype = {arxiv},
	eprint = {2212.09462 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, {CT}-read, Collaboration},
	file = {Preprint PDF:/Users/moonshine/Zotero/storage/WE68ULRN/Lovelace et al. - 2023 - Latent Diffusion for Language Generation.pdf:application/pdf;Snapshot:/Users/moonshine/Zotero/storage/8XQFBQEU/2212.html:text/html},
}

@misc{xu_energy-based_2025,
	title = {Energy-Based Diffusion Language Models for Text Generation},
	url = {http://arxiv.org/abs/2410.21357},
	doi = {10.48550/arXiv.2410.21357},
	abstract = {Despite remarkable progress in autoregressive language models, alternative generative paradigms beyond left-to-right generation are still being actively explored. Discrete diffusion models, with the capacity for parallel generation, have recently emerged as a promising alternative. Unfortunately, these models still underperform the autoregressive counterparts, with the performance gap increasing when reducing the number of sampling steps. Our analysis reveals that this degradation is a consequence of an imperfect approximation used by diffusion models. In this work, we propose Energy-based Diffusion Language Model ({EDLM}), an energy-based model operating at the full sequence level for each diffusion step, introduced to improve the underlying approximation used by diffusion models. More specifically, we introduce an {EBM} in a residual form, and show that its parameters can be obtained by leveraging a pretrained autoregressive model or by finetuning a bidirectional transformer via noise contrastive estimation. We also propose an efficient generation algorithm via parallel important sampling. Comprehensive experiments on language modeling benchmarks show that our model can consistently outperform state-of-the-art diffusion models by a significant margin, and approaches autoregressive models' perplexity. We further show that, without any generation performance drop, our framework offers a 1.3\${\textbackslash}times\$ sampling speedup over existing diffusion models. Reproduced code is available at https://github.com/{MinkaiXu}/Energy-Diffusion-{LLM}.},
	number = {{arXiv}:2410.21357},
	publisher = {{arXiv}},
	author = {Xu, Minkai and Geffner, Tomas and Kreis, Karsten and Nie, Weili and Xu, Yilun and Leskovec, Jure and Ermon, Stefano and Vahdat, Arash},
	urldate = {2025-04-23},
	date = {2025-03-07},
	eprinttype = {arxiv},
	eprint = {2410.21357 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, Value, {CT}-read, {InferenceSpeed}, Collaboration, Theory},
	file = {Preprint PDF:/Users/moonshine/Zotero/storage/KFLHQDBN/Xu et al. - 2025 - Energy-Based Diffusion Language Models for Text Generation.pdf:application/pdf;Snapshot:/Users/moonshine/Zotero/storage/LWYZJU35/2410.html:text/html},
}

@misc{liu_efficient_2025,
	title = {Efficient Inference for Large Reasoning Models: A Survey},
	url = {http://arxiv.org/abs/2503.23077},
	doi = {10.48550/arXiv.2503.23077},
	shorttitle = {Efficient Inference for Large Reasoning Models},
	abstract = {Large Reasoning Models ({LRMs}) significantly improve the reasoning ability of Large Language Models ({LLMs}) by learning to reason, exhibiting promising performance in complex task-solving. However, their deliberative reasoning process leads to inefficiencies in token usage, memory consumption, and inference time. Thus, this survey provides a review of efficient inference methods designed specifically for {LRMs}, focusing on mitigating token inefficiency while preserving the reasoning quality. First, we introduce a taxonomy to group the recent methods into two main categories: (a) explicit compact Chain-of-Thought ({CoT}), which reduces tokens while keeping the explicit reasoning structure, and (b) implicit latent {CoT}, which encodes reasoning steps within hidden representations instead of explicit tokens. Meanwhile, we discuss their strengths and weaknesses. Then, we conduct empirical analyses on existing methods from performance and efficiency aspects. Besides, we present open challenges in this field, including human-centric controllable reasoning, trade-off between interpretability and efficiency of reasoning, ensuring safety of efficient reasoning, and broader applications of efficient reasoning. In addition, we highlight key insights for enhancing {LRMs}' inference efficiency via techniques such as model merging, new architectures, and agent routers. We hope this work serves as a valuable guide, helping researchers overcome challenges in this vibrant field{\textbackslash}footnote\{https://github.com/yueliu1999/Awesome-Efficient-Inference-for-{LRMs}\}.},
	number = {{arXiv}:2503.23077},
	publisher = {{arXiv}},
	author = {Liu, Yue and Wu, Jiaying and He, Yufei and Gao, Hongcheng and Chen, Hongyu and Bi, Baolong and Zhang, Jiaheng and Huang, Zhiqi and Hooi, Bryan},
	urldate = {2025-04-23},
	date = {2025-03-29},
	eprinttype = {arxiv},
	eprint = {2503.23077},
	keywords = {Computer Science - Computation and Language, {CT}-read, Collaboration},
	file = {Efficient Inference for Large Reasoning Models A Survey.pdf:/Users/moonshine/Zotero/storage/GVSEFPBV/Efficient Inference for Large Reasoning Models A Survey.pdf:application/pdf},
}

@misc{fathi_unifying_2025,
	title = {Unifying Autoregressive and Diffusion-Based Sequence Generation},
	url = {http://arxiv.org/abs/2504.06416},
	doi = {10.48550/arXiv.2504.06416},
	abstract = {We present significant extensions to diffusion-based sequence generation models, blurring the line with autoregressive language models. We introduce hyperschedules, which assign distinct noise schedules to individual token positions, generalizing both autoregressive models (e.g., {GPT}) and conventional diffusion models (e.g., {SEDD}, {MDLM}) as special cases. Second, we propose two hybrid token-wise noising processes that interpolate between absorbing and uniform processes, enabling the model to fix past mistakes, and we introduce a novel inference algorithm that leverages this new feature in a simplified context inspired from {MDLM}. To support efficient training and inference, we design attention masks compatible with {KV}-caching. Our methods achieve state-of-the-art perplexity and generate diverse, high-quality sequences across standard benchmarks, suggesting a promising path for autoregressive diffusion-based sequence generation.},
	number = {{arXiv}:2504.06416},
	publisher = {{arXiv}},
	author = {Fathi, Nima and Scholak, Torsten and Noël, Pierre-André},
	urldate = {2025-04-23},
	date = {2025-04-08},
	eprinttype = {arxiv},
	eprint = {2504.06416},
	keywords = {Computer Science - Machine Learning, Value, {CT}-read, Interporablity, Eric-read},
	file = {Unifying Autoregressive and Diffusion-Based Sequence Generation.pdf:/Users/moonshine/Zotero/storage/FJXWBT8V/Unifying Autoregressive and Diffusion-Based Sequence Generation.pdf:application/pdf},
}

@misc{yang_using_2024,
	title = {Using Human Feedback to Fine-tune Diffusion Models without Any Reward Model},
	url = {http://arxiv.org/abs/2311.13231},
	doi = {10.48550/arXiv.2311.13231},
	abstract = {Using reinforcement learning with human feedback ({RLHF}) has shown significant promise in fine-tuning diffusion models. Previous methods start by training a reward model that aligns with human preferences, then leverage {RL} techniques to fine-tune the underlying models. However, crafting an efficient reward model demands extensive datasets, optimal architecture, and manual hyperparameter tuning, making the process both time and cost-intensive. The direct preference optimization ({DPO}) method, effective in fine-tuning large language models, eliminates the necessity for a reward model. However, the extensive {GPU} memory requirement of the diffusion model's denoising process hinders the direct application of the {DPO} method. To address this issue, we introduce the Direct Preference for Denoising Diffusion Policy Optimization (D3PO) method to directly fine-tune diffusion models. The theoretical analysis demonstrates that although D3PO omits training a reward model, it effectively functions as the optimal reward model trained using human feedback data to guide the learning process. This approach requires no training of a reward model, proving to be more direct, cost-effective, and minimizing computational overhead. In experiments, our method uses the relative scale of objectives as a proxy for human preference, delivering comparable results to methods using ground-truth rewards. Moreover, D3PO demonstrates the ability to reduce image distortion rates and generate safer images, overcoming challenges lacking robust reward models. Our code is publicly available at https://github.com/yk7333/D3PO.},
	number = {{arXiv}:2311.13231},
	publisher = {{arXiv}},
	author = {Yang, Kai and Tao, Jian and Lyu, Jiafei and Ge, Chunjiang and Chen, Jiaxin and Li, Qimai and Shen, Weihan and Zhu, Xiaolong and Li, Xiu},
	urldate = {2025-04-24},
	date = {2024-03-23},
	eprinttype = {arxiv},
	eprint = {2311.13231 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, {CT}-read},
	file = {Full Text PDF:/Users/moonshine/Zotero/storage/57Q56AEH/Yang et al. - 2024 - Using Human Feedback to Fine-tune Diffusion Models without Any Reward Model.pdf:application/pdf;Snapshot:/Users/moonshine/Zotero/storage/T9GWP3PT/2311.html:text/html},
}

@misc{han_ssd-lm_2023,
	title = {{SSD}-{LM}: Semi-autoregressive Simplex-based Diffusion Language Model for Text Generation and Modular Control},
	url = {http://arxiv.org/abs/2210.17432},
	doi = {10.48550/arXiv.2210.17432},
	shorttitle = {{SSD}-{LM}},
	abstract = {Despite the growing success of diffusion models in continuous-valued domains (e.g., images), similar efforts for discrete domains such as text have yet to match the performance of autoregressive language models. In this work, we present {SSD}-{LM} -- a diffusion-based language model with two key design choices. First, {SSD}-{LM} is semi-autoregressive, iteratively generating blocks of text, allowing for flexible output length at decoding time while enabling local bidirectional context updates. Second, it is simplex-based, performing diffusion on the natural vocabulary space rather than a learned latent space, allowing us to incorporate classifier guidance and modular control using off-the-shelf classifiers without any adaptation. We evaluate {SSD}-{LM} on unconstrained text generation benchmarks, and show that it matches or outperforms strong autoregressive {GPT}-2 models across standard quality and diversity metrics, while vastly outperforming diffusion-based baselines. On controlled text generation, {SSD}-{LM} also outperforms competitive baselines, with an extra advantage in modularity.},
	number = {{arXiv}:2210.17432},
	publisher = {{arXiv}},
	author = {Han, Xiaochuang and Kumar, Sachin and Tsvetkov, Yulia},
	urldate = {2025-04-24},
	date = {2023-06-26},
	eprinttype = {arxiv},
	eprint = {2210.17432 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, Application},
	file = {Full Text PDF:/Users/moonshine/Zotero/storage/DQ5J7HQA/Han et al. - 2023 - SSD-LM Semi-autoregressive Simplex-based Diffusion Language Model for Text Generation and Modular C.pdf:application/pdf;Snapshot:/Users/moonshine/Zotero/storage/VGMKSZUN/2210.html:text/html},
}

@misc{sahoo_simple_2024,
	title = {Simple and Effective Masked Diffusion Language Models},
	url = {http://arxiv.org/abs/2406.07524},
	doi = {10.48550/arXiv.2406.07524},
	abstract = {While diffusion models excel at generating high-quality images, prior work reports a significant performance gap between diffusion and autoregressive ({AR}) methods in language modeling. In this work, we show that simple masked discrete diffusion is more performant than previously thought. We apply an effective training recipe that improves the performance of masked diffusion models and derive a simplified, Rao-Blackwellized objective that results in additional improvements. Our objective has a simple form -- it is a mixture of classical masked language modeling losses -- and can be used to train encoder-only language models that admit efficient samplers, including ones that can generate arbitrary lengths of text semi-autoregressively like a traditional language model. On language modeling benchmarks, a range of masked diffusion models trained with modern engineering practices achieves a new state-of-the-art among diffusion models, and approaches {AR} perplexity. We provide the code, along with a blog post and video tutorial on the project page: https://s-sahoo.com/mdlm},
	number = {{arXiv}:2406.07524},
	publisher = {{arXiv}},
	author = {Sahoo, Subham Sekhar and Arriola, Marianne and Schiff, Yair and Gokaslan, Aaron and Marroquin, Edgar and Chiu, Justin T. and Rush, Alexander and Kuleshov, Volodymyr},
	urldate = {2025-04-24},
	date = {2024-11-10},
	eprinttype = {arxiv},
	eprint = {2406.07524 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, {CT}-read, Application},
	file = {Full Text PDF:/Users/moonshine/Zotero/storage/T89ZMQ7S/Sahoo et al. - 2024 - Simple and Effective Masked Diffusion Language Models.pdf:application/pdf;Snapshot:/Users/moonshine/Zotero/storage/YFM3D299/2406.html:text/html},
}

@misc{lin_text_2023,
	title = {Text Generation with Diffusion Language Models: A Pre-training Approach with Continuous Paragraph Denoise},
	url = {http://arxiv.org/abs/2212.11685},
	doi = {10.48550/arXiv.2212.11685},
	shorttitle = {Text Generation with Diffusion Language Models},
	abstract = {In this paper, we introduce a novel {dIffusion} language {modEl} pre-training framework for text generation, which we call {GENIE}. {GENIE} is a large-scale pretrained diffusion language model that consists of an encoder and a diffusion-based decoder, which can generate text by gradually transforming a random noise sequence into a coherent text sequence. To pre-train {GENIE} on a large-scale language corpus, we design a new continuous paragraph denoise objective, which encourages the diffusion-decoder to reconstruct a clean text paragraph from a corrupted version, while preserving the semantic and syntactic coherence. We evaluate {GENIE} on four downstream text generation benchmarks, namely {XSum}, {CNN}/{DailyMail}, Gigaword, and {CommonGen}. Our experimental results show that {GENIE} achieves comparable performance with the state-of-the-art autoregressive models on these benchmarks, and generates more diverse text samples. The code and models of {GENIE} are available at https://github.com/microsoft/{ProphetNet}/tree/master/{GENIE}.},
	number = {{arXiv}:2212.11685},
	publisher = {{arXiv}},
	author = {Lin, Zhenghao and Gong, Yeyun and Shen, Yelong and Wu, Tong and Fan, Zhihao and Lin, Chen and Duan, Nan and Chen, Weizhu},
	urldate = {2025-04-24},
	date = {2023-02-17},
	langid = {american},
	eprinttype = {arxiv},
	eprint = {2212.11685 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, {CT}-read, Application, Eric-read},
	file = {Full Text PDF:/Users/moonshine/Zotero/storage/DGI28U5M/Lin et al. - 2023 - Text Generation with Diffusion Language Models A Pre-training Approach with Continuous Paragraph De.pdf:application/pdf;Snapshot:/Users/moonshine/Zotero/storage/Z555YRZN/2212.html:text/html},
}

@misc{zhang_planner_2024,
	title = {{PLANNER}: Generating Diversified Paragraph via Latent Language Diffusion Model},
	url = {http://arxiv.org/abs/2306.02531},
	doi = {10.48550/arXiv.2306.02531},
	shorttitle = {{PLANNER}},
	abstract = {Autoregressive models for text sometimes generate repetitive and low-quality output because errors accumulate during the steps of generation. This issue is often attributed to exposure bias - the difference between how a model is trained, and how it is used during inference. Denoising diffusion models provide an alternative approach in which a model can revisit and revise its output. However, they can be computationally expensive and prior efforts on text have led to models that produce less fluent output compared to autoregressive models, especially for longer text and paragraphs. In this paper, we propose {PLANNER}, a model that combines latent semantic diffusion with autoregressive generation, to generate fluent text while exercising global control over paragraphs. The model achieves this by combining an autoregressive "decoding" module with a "planning" module that uses latent diffusion to generate semantic paragraph embeddings in a coarse-to-fine manner. The proposed method is evaluated on various conditional generation tasks, and results on semantic generation, text completion and summarization show its effectiveness in generating high-quality long-form text in an efficient manner.},
	number = {{arXiv}:2306.02531},
	publisher = {{arXiv}},
	author = {Zhang, Yizhe and Gu, Jiatao and Wu, Zhuofeng and Zhai, Shuangfei and Susskind, Josh and Jaitly, Navdeep},
	urldate = {2025-04-24},
	date = {2024-03-22},
	eprinttype = {arxiv},
	eprint = {2306.02531 [cs]},
	keywords = {Computer Science - Computation and Language, Application},
	file = {Full Text PDF:/Users/moonshine/Zotero/storage/ZJIUVRRQ/Zhang et al. - 2024 - PLANNER Generating Diversified Paragraph via Latent Language Diffusion Model.pdf:application/pdf;Snapshot:/Users/moonshine/Zotero/storage/6Z6XSDFI/2306.html:text/html},
}

@misc{ma_exploring_2024,
	title = {Exploring the Role of Large Language Models in Prompt Encoding for Diffusion Models},
	url = {http://arxiv.org/abs/2406.11831},
	doi = {10.48550/arXiv.2406.11831},
	abstract = {Large language models ({LLMs}) based on decoder-only transformers have demonstrated superior text understanding capabilities compared to {CLIP} and T5-series models. However, the paradigm for utilizing current advanced {LLMs} in text-to-image diffusion models remains to be explored. We observed an unusual phenomenon: directly using a large language model as the prompt encoder significantly degrades the prompt-following ability in image generation. We identified two main obstacles behind this issue. One is the misalignment between the next token prediction training in {LLM} and the requirement for discriminative prompt features in diffusion models. The other is the intrinsic positional bias introduced by the decoder-only architecture. To deal with this issue, we propose a novel framework to fully harness the capabilities of {LLMs}. Through the carefully designed usage guidance, we effectively enhance the text representation capability for prompt encoding and eliminate its inherent positional bias. This allows us to integrate state-of-the-art {LLMs} into the text-to-image generation model flexibly. Furthermore, we also provide an effective manner to fuse multiple {LLMs} into our framework. Considering the excellent performance and scaling capabilities demonstrated by the transformer architecture, we further design an {LLM}-Infused Diffusion Transformer ({LI}-{DiT}) based on the framework. We conduct extensive experiments to validate {LI}-{DiT} across model size and data size. Benefiting from the inherent ability of the {LLMs} and our innovative designs, the prompt understanding performance of {LI}-{DiT} easily surpasses state-of-the-art open-source models as well as mainstream closed-source commercial models including Stable Diffusion 3, {DALL}-E 3, and Midjourney V6. The {LLM}-Infused Diffuser framework is also one of the core technologies powering {SenseMirage}, a highly advanced text-to-image model.},
	number = {{arXiv}:2406.11831},
	publisher = {{arXiv}},
	author = {Ma, Bingqi and Zong, Zhuofan and Song, Guanglu and Li, Hongsheng and Liu, Yu},
	urldate = {2025-04-24},
	date = {2024-12-05},
	eprinttype = {arxiv},
	eprint = {2406.11831 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, {CT}-read, Collaboration, Application},
	file = {Full Text PDF:/Users/moonshine/Zotero/storage/S7AYDIKM/Ma et al. - 2024 - Exploring the Role of Large Language Models in Prompt Encoding for Diffusion Models.pdf:application/pdf;Snapshot:/Users/moonshine/Zotero/storage/7RRAACJY/2406.html:text/html},
}

@misc{wang_dplm-2_2024,
	title = {{DPLM}-2: A Multimodal Diffusion Protein Language Model},
	url = {http://arxiv.org/abs/2410.13782},
	doi = {10.48550/arXiv.2410.13782},
	shorttitle = {{DPLM}-2},
	abstract = {Proteins are essential macromolecules defined by their amino acid sequences, which determine their three-dimensional structures and, consequently, their functions in all living organisms. Therefore, generative protein modeling necessitates a multimodal approach to simultaneously model, understand, and generate both sequences and structures. However, existing methods typically use separate models for each modality, limiting their ability to capture the intricate relationships between sequence and structure. This results in suboptimal performance in tasks that requires joint understanding and generation of both modalities. In this paper, we introduce {DPLM}-2, a multimodal protein foundation model that extends discrete diffusion protein language model ({DPLM}) to accommodate both sequences and structures. To enable structural learning with the language model, 3D coordinates are converted to discrete tokens using a lookup-free quantization-based tokenizer. By training on both experimental and high-quality synthetic structures, {DPLM}-2 learns the joint distribution of sequence and structure, as well as their marginals and conditionals. We also implement an efficient warm-up strategy to exploit the connection between large-scale evolutionary data and structural inductive biases from pre-trained sequence-based protein language models. Empirical evaluation shows that {DPLM}-2 can simultaneously generate highly compatible amino acid sequences and their corresponding 3D structures eliminating the need for a two-stage generation approach. Moreover, {DPLM}-2 demonstrates competitive performance in various conditional generation tasks, including folding, inverse folding, and scaffolding with multimodal motif inputs, as well as providing structure-aware representations for predictive tasks.},
	number = {{arXiv}:2410.13782},
	publisher = {{arXiv}},
	author = {Wang, Xinyou and Zheng, Zaixiang and Ye, Fei and Xue, Dongyu and Huang, Shujian and Gu, Quanquan},
	urldate = {2025-04-24},
	date = {2024-10-17},
	eprinttype = {arxiv},
	eprint = {2410.13782 [cs]},
	keywords = {Computer Science - Machine Learning, Quantitative Biology - Quantitative Methods, {CT}-read, Application},
	file = {Full Text PDF:/Users/moonshine/Zotero/storage/C2M2ULUE/Wang et al. - 2024 - DPLM-2 A Multimodal Diffusion Protein Language Model.pdf:application/pdf;Snapshot:/Users/moonshine/Zotero/storage/BQL6F5C5/2410.html:text/html},
}

@misc{meshchaninov_diffusion_2025,
	title = {Diffusion on language model encodings for protein sequence generation},
	url = {http://arxiv.org/abs/2403.03726},
	doi = {10.48550/arXiv.2403.03726},
	abstract = {Protein sequence design has seen significant advances through discrete diffusion and autoregressive approaches, yet the potential of continuous diffusion remains underexplored. Here, we present {DiMA}, a latent diffusion framework that operates on protein language model representations. Through systematic exploration of architectural choices and diffusion components, we develop a robust methodology that generalizes across multiple protein encoders ranging from 8M to 3B parameters. We demonstrate that our framework achieves consistently high performance across sequence-only ({ESM}-2, {ESMc}), dual-decodable ({CHEAP}), and multimodal ({SaProt}) representations using the same architecture and training approach. We extensively evaluate existing methods alongside {DiMA} using multiple metrics across two protein modalities, covering quality, diversity, novelty, and distribution matching of generated proteins. {DiMA} consistently produces novel, high-quality and diverse protein sequences and achieves strong results compared to baselines such as autoregressive, discrete diffusion and flow matching language models. The model demonstrates versatile functionality, supporting conditional generation tasks including protein family-generation, motif scaffolding and infilling, and fold-specific sequence design. This work provides a universal continuous diffusion framework for protein sequence generation, offering both architectural insights and practical applicability across various protein design scenarios.},
	number = {{arXiv}:2403.03726},
	publisher = {{arXiv}},
	author = {Meshchaninov, Viacheslav and Strashnov, Pavel and Shevtsov, Andrey and Nikolaev, Fedor and Ivanisenko, Nikita and Kardymon, Olga and Vetrov, Dmitry},
	urldate = {2025-04-24},
	date = {2025-02-05},
	eprinttype = {arxiv},
	eprint = {2403.03726 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Quantitative Biology - Biomolecules, Application},
	file = {Full Text PDF:/Users/moonshine/Zotero/storage/X9U6P2JK/Meshchaninov et al. - 2025 - Diffusion on language model encodings for protein sequence generation.pdf:application/pdf;Snapshot:/Users/moonshine/Zotero/storage/9NRA8E5A/2403.html:text/html},
}

@misc{luo_deem_2025,
	title = {{DEEM}: Diffusion Models Serve as the Eyes of Large Language Models for Image Perception},
	url = {http://arxiv.org/abs/2405.15232},
	doi = {10.48550/arXiv.2405.15232},
	shorttitle = {{DEEM}},
	abstract = {The development of large language models ({LLMs}) has significantly advanced the emergence of large multimodal models ({LMMs}). While {LMMs} have achieved tremendous success by promoting the synergy between multimodal comprehension and creation, they often face challenges when confronted with out-of-distribution data, such as which can hardly distinguish orientation, quantity, color, structure, etc. This is primarily due to their reliance on image encoders trained to encode images into task-relevant features, which may lead them to disregard irrelevant details. Delving into the modeling capabilities of diffusion models for images naturally prompts the question: Can diffusion models serve as the eyes of large language models for image perception? In this paper, we propose {DEEM}, a simple but effective approach that utilizes the generative feedback of diffusion models to align the semantic distributions of the image encoder. This addresses the drawbacks of previous methods that solely relied on image encoders like {CLIP}-{ViT}, thereby enhancing the model's resilience against out-of-distribution samples and reducing visual hallucinations. Importantly, this is achieved without requiring additional training modules and with fewer training parameters. We extensively evaluated {DEEM} on both our newly constructed {RobustVQA} benchmark and other well-known benchmarks, {POPE} and {MMVP}, for visual hallucination and perception. In particular, {DEEM} improves {LMM}'s visual perception performance to a large extent (e.g., 4\% higher on {RobustVQA}, 6.5\% higher on {MMVP} and 12.8 \% higher on {POPE} ). Compared to the state-of-the-art interleaved content generation models, {DEEM} exhibits enhanced robustness and a superior capacity to alleviate model hallucinations while utilizing fewer trainable parameters, less pre-training data (10\%), and a smaller base model size.},
	number = {{arXiv}:2405.15232},
	publisher = {{arXiv}},
	author = {Luo, Run and Li, Yunshui and Chen, Longze and He, Wanwei and Lin, Ting-En and Liu, Ziqiang and Zhang, Lei and Song, Zikai and Xia, Xiaobo and Liu, Tongliang and Yang, Min and Hui, Binyuan},
	urldate = {2025-04-24},
	date = {2025-03-08},
	eprinttype = {arxiv},
	eprint = {2405.15232 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Computation and Language, {CT}-read},
	file = {Full Text PDF:/Users/moonshine/Zotero/storage/2GWE4T4E/Luo et al. - 2025 - DEEM Diffusion Models Serve as the Eyes of Large Language Models for Image Perception.pdf:application/pdf;Snapshot:/Users/moonshine/Zotero/storage/2VTJT693/2405.html:text/html},
}

@misc{yang_diffusion_2024,
	title = {Diffusion Models: A Comprehensive Survey of Methods and Applications},
	url = {http://arxiv.org/abs/2209.00796},
	doi = {10.48550/arXiv.2209.00796},
	shorttitle = {Diffusion Models},
	abstract = {Diffusion models have emerged as a powerful new family of deep generative models with record-breaking performance in many applications, including image synthesis, video generation, and molecule design. In this survey, we provide an overview of the rapidly expanding body of work on diffusion models, categorizing the research into three key areas: efficient sampling, improved likelihood estimation, and handling data with special structures. We also discuss the potential for combining diffusion models with other generative models for enhanced results. We further review the wide-ranging applications of diffusion models in fields spanning from computer vision, natural language generation, temporal data modeling, to interdisciplinary applications in other scientific disciplines. This survey aims to provide a contextualized, in-depth look at the state of diffusion models, identifying the key areas of focus and pointing to potential areas for further exploration. Github: https://github.com/{YangLing}0818/Diffusion-Models-Papers-Survey-Taxonomy.},
	number = {{arXiv}:2209.00796},
	publisher = {{arXiv}},
	author = {Yang, Ling and Zhang, Zhilong and Song, Yang and Hong, Shenda and Xu, Runsheng and Zhao, Yue and Zhang, Wentao and Cui, Bin and Yang, Ming-Hsuan},
	urldate = {2025-04-24},
	date = {2024-12-02},
	eprinttype = {arxiv},
	eprint = {2209.00796 [cs]},
	keywords = {Literature Review, Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, {CT}-read, Application},
	file = {Full Text PDF:/Users/moonshine/Zotero/storage/XL2HYS54/Yang et al. - 2024 - Diffusion Models A Comprehensive Survey of Methods and Applications.pdf:application/pdf;Snapshot:/Users/moonshine/Zotero/storage/GJA6JBDP/2209.html:text/html},
}

@misc{shabalin_tencdm_2025,
	title = {{TEncDM}: Understanding the Properties of the Diffusion Model in the Space of Language Model Encodings},
	url = {http://arxiv.org/abs/2402.19097},
	doi = {10.48550/arXiv.2402.19097},
	shorttitle = {{TEncDM}},
	abstract = {This paper presents the Text Encoding Diffusion Model ({TEncDM}), a novel approach to diffusion modeling that operates in the space of pre-trained language model encodings. In contrast to traditionally used embeddings, encodings integrate contextual information. In our approach, we also employ a transformer-based decoder, specifically designed to incorporate context in the token prediction process. We conduct a comprehensive examination of the influence of the encoder, decoder, noise scheduler, and self-conditioning on zero-shot generation. Furthermore, we compare {TEncDM} with previous approaches on three conditional text generation tasks: {QQP}, {XSum}, and Wiki-Auto. The results show that {TEncDM} exhibits superior performance compared to existing non-autoregressive diffusion models. Our code is available at https://github.com/M0RJIQUE/tencdm.},
	number = {{arXiv}:2402.19097},
	publisher = {{arXiv}},
	author = {Shabalin, Alexander and Meshchaninov, Viacheslav and Chimbulatov, Egor and Lapikov, Vladislav and Kim, Roman and Bartosh, Grigory and Molchanov, Dmitry and Markov, Sergey and Vetrov, Dmitry},
	urldate = {2025-04-24},
	date = {2025-02-24},
	eprinttype = {arxiv},
	eprint = {2402.19097 [cs]},
	keywords = {Computer Science - Computation and Language, {CT}-read, Collaboration, Application},
	file = {Full Text PDF:/Users/moonshine/Zotero/storage/TPPYFYRY/Shabalin et al. - 2025 - TEncDM Understanding the Properties of the Diffusion Model in the Space of Language Model Encodings.pdf:application/pdf;Snapshot:/Users/moonshine/Zotero/storage/ZIG98DUW/2402.html:text/html},
}

@misc{zhu_diffusion_2023,
	title = {Diffusion Models in {NLP}: A Survey},
	url = {http://arxiv.org/abs/2303.07576},
	doi = {10.48550/arXiv.2303.07576},
	shorttitle = {Diffusion Models in {NLP}},
	abstract = {Diffusion models have become a powerful family of deep generative models, with record-breaking performance in many applications. This paper first gives an overview and derivation of the basic theory of diffusion models, then reviews the research results of diffusion models in the field of natural language processing, from text generation, text-driven image generation and other four aspects, and analyzes and summarizes the relevant literature materials sorted out, and finally records the experience and feelings of this topic literature review research.},
	number = {{arXiv}:2303.07576},
	publisher = {{arXiv}},
	author = {Zhu, Yuansong and Zhao, Yu},
	urldate = {2025-04-24},
	date = {2023-03-14},
	eprinttype = {arxiv},
	eprint = {2303.07576 [cs]},
	keywords = {Literature Review, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, {CT}-read, Interporablity},
	file = {Preprint PDF:/Users/moonshine/Zotero/storage/HHB74M95/Zhu and Zhao - 2023 - Diffusion Models in NLP A Survey.pdf:application/pdf},
}

@misc{deschenaux_promises_2024,
	title = {Promises, Outlooks and Challenges of Diffusion Language Modeling},
	url = {http://arxiv.org/abs/2406.11473},
	doi = {10.48550/arXiv.2406.11473},
	abstract = {The modern autoregressive Large Language Models ({LLMs}) have achieved outstanding performance on {NLP} benchmarks, and they are deployed in the real world. However, they still suffer from limitations of the autoregressive training paradigm. For example, autoregressive token generation is notably slow and can be prone to {\textbackslash}textit\{exposure bias\}. The diffusion-based language models were proposed as an alternative to autoregressive generation to address some of these limitations. We evaluate the recently proposed Score Entropy Discrete Diffusion ({SEDD}) approach and show it is a promising alternative to autoregressive generation but it has some short-comings too. We empirically demonstrate the advantages and challenges of {SEDD}, and observe that {SEDD} generally matches autoregressive models in perplexity and on benchmarks such as {HellaSwag}, Arc or {WinoGrande}. Additionally, we show that in terms of inference latency, {SEDD} can be up to 4.5\${\textbackslash}times\$ more efficient than {GPT}-2. While {SEDD} allows conditioning on tokens at abitrary positions, {SEDD} appears slightly weaker than {GPT}-2 for conditional generation given short prompts. Finally, we reproduced the main results from the original {SEDD} paper.},
	number = {{arXiv}:2406.11473},
	publisher = {{arXiv}},
	author = {Deschenaux, Justin and Gulcehre, Caglar},
	urldate = {2025-04-24},
	date = {2024-07-10},
	eprinttype = {arxiv},
	eprint = {2406.11473 [cs]},
	keywords = {Literature Review, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, {CT}-read},
	file = {Full Text PDF:/Users/moonshine/Zotero/storage/MB92P3B8/Deschenaux and Gulcehre - 2024 - Promises, Outlooks and Challenges of Diffusion Language Modeling.pdf:application/pdf;Snapshot:/Users/moonshine/Zotero/storage/SNCZE5E7/2406.html:text/html},
}

@misc{zhang_artist_2024,
	title = {{ARTIST}: Improving the Generation of Text-rich Images with Disentangled Diffusion Models and Large Language Models},
	url = {http://arxiv.org/abs/2406.12044},
	doi = {10.48550/arXiv.2406.12044},
	shorttitle = {{ARTIST}},
	abstract = {Diffusion models have demonstrated exceptional capabilities in generating a broad spectrum of visual content, yet their proficiency in rendering text is still limited: they often generate inaccurate characters or words that fail to blend well with the underlying image. To address these shortcomings, we introduce a novel framework named, {ARTIST}, which incorporates a dedicated textual diffusion model to focus on the learning of text structures specifically. Initially, we pretrain this textual model to capture the intricacies of text representation. Subsequently, we finetune a visual diffusion model, enabling it to assimilate textual structure information from the pretrained textual model. This disentangled architecture design and training strategy significantly enhance the text rendering ability of the diffusion models for text-rich image generation. Additionally, we leverage the capabilities of pretrained large language models to interpret user intentions better, contributing to improved generation quality. Empirical results on the {MARIO}-Eval benchmark underscore the effectiveness of the proposed method, showing an improvement of up to 15\% in various metrics.},
	number = {{arXiv}:2406.12044},
	publisher = {{arXiv}},
	author = {Zhang, Jianyi and Zhou, Yufan and Gu, Jiuxiang and Wigington, Curtis and Yu, Tong and Chen, Yiran and Sun, Tong and Zhang, Ruiyi},
	urldate = {2025-04-24},
	date = {2024-12-02},
	eprinttype = {arxiv},
	eprint = {2406.12044 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Multi-modal, {CT}-read, Application},
	file = {Full Text PDF:/Users/moonshine/Zotero/storage/RQGGDX7W/Zhang et al. - 2024 - ARTIST Improving the Generation of Text-rich Images with Disentangled Diffusion Models and Large La.pdf:application/pdf;Snapshot:/Users/moonshine/Zotero/storage/Q4U66AJ5/2406.html:text/html},
}

@misc{han_david_2024,
	title = {David helps Goliath: Inference-Time Collaboration Between Small Specialized and Large General Diffusion {LMs}},
	url = {http://arxiv.org/abs/2305.14771},
	doi = {10.48550/arXiv.2305.14771},
	shorttitle = {David helps Goliath},
	abstract = {Diffusion-based language models are emerging as a promising alternative to autoregressive {LMs}: they approach the competence of autoregressive {LMs} while offering nuanced controllability at inference time. While autoregressive {LMs} have benefited immensely from scaling and instruction-based learning, existing studies of diffusion {LMs} have been conducted on a smaller scale. Starting with a recently proposed diffusion model {SSD}-{LM}, in this work we first explore methods to scale it from 0.4B to 13B parameters, proposing techniques to improve its training and inference efficiency, and to finetune the model to follow instructions. Armed with a more powerful, general purpose diffusion {LM}, we introduce the primary contribution of this work -- {SSD}-2 -- an approach to easily ensemble at inference time a large general-purpose diffusion {LM} with smaller, but specialized and contextualized diffusion {LMs}. We show that {SSD}-2 facilitates novel ensembles with 100x smaller models that can be customized and deployed by individual users. We find that compared to autoregressive models, the collaboration between diffusion {LMs} is more effective, leading to higher-quality model responses due to their ability to dynamically incorporate bi-directional contexts.},
	number = {{arXiv}:2305.14771},
	publisher = {{arXiv}},
	author = {Han, Xiaochuang and Kumar, Sachin and Tsvetkov, Yulia and Ghazvininejad, Marjan},
	urldate = {2025-04-24},
	date = {2024-02-14},
	eprinttype = {arxiv},
	eprint = {2305.14771 [cs]},
	keywords = {Computer Science - Computation and Language, Value, {CT}-read, {InferenceSpeed}, Collaboration, Application},
	file = {Full Text PDF:/Users/moonshine/Zotero/storage/YS4FX74L/Han et al. - 2024 - David helps Goliath Inference-Time Collaboration Between Small Specialized and Large General Diffus.pdf:application/pdf;Snapshot:/Users/moonshine/Zotero/storage/BVCLDIXW/2305.html:text/html},
}

@misc{wu_ar-diffusion_2023,
	title = {{AR}-Diffusion: Auto-Regressive Diffusion Model for Text Generation},
	url = {http://arxiv.org/abs/2305.09515},
	doi = {10.48550/arXiv.2305.09515},
	shorttitle = {{AR}-Diffusion},
	abstract = {Diffusion models have gained significant attention in the realm of image generation due to their exceptional performance. Their success has been recently expanded to text generation via generating all tokens within a sequence concurrently. However, natural language exhibits a far more pronounced sequential dependency in comparison to images, and the majority of existing language models are trained with a left-to-right auto-regressive approach. To account for the inherent sequential characteristic of natural language, we introduce Auto-Regressive Diffusion ({AR}-Diffusion). {AR}-Diffusion ensures that the generation of tokens on the right depends on the generated ones on the left, a mechanism achieved through employing a dynamic number of denoising steps that vary based on token position. This results in tokens on the left undergoing fewer denoising steps than those on the right, thereby enabling them to generate earlier and subsequently influence the generation of tokens on the right. In a series of experiments on various text generation tasks, including text summarization, machine translation, and common sense generation, {AR}-Diffusion clearly demonstrated its superiority over existing diffusion language models and that it can be \$100{\textbackslash}times{\textbackslash}sim600{\textbackslash}times\$ faster when achieving comparable results. Our code is available at https://github.com/microsoft/{ProphetNet}/tree/master/{AR}-diffusion.},
	number = {{arXiv}:2305.09515},
	publisher = {{arXiv}},
	author = {Wu, Tong and Fan, Zhihao and Liu, Xiao and Gong, Yeyun and Shen, Yelong and Jiao, Jian and Zheng, Hai-Tao and Li, Juntao and Wei, Zhongyu and Guo, Jian and Duan, Nan and Chen, Weizhu},
	urldate = {2025-04-24},
	date = {2023-12-13},
	eprinttype = {arxiv},
	eprint = {2305.09515 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, {CT}-read, {InferenceSpeed}, Application},
	file = {Full Text PDF:/Users/moonshine/Zotero/storage/A33MRBMX/Wu et al. - 2023 - AR-Diffusion Auto-Regressive Diffusion Model for Text Generation.pdf:application/pdf;Snapshot:/Users/moonshine/Zotero/storage/8WALEIFC/2305.html:text/html},
}

@misc{jo_continuous_2025,
	title = {Continuous Diffusion Model for Language Modeling},
	url = {http://arxiv.org/abs/2502.11564},
	doi = {10.48550/arXiv.2502.11564},
	abstract = {Diffusion models have emerged as a promising alternative to autoregressive models in modeling discrete categorical data. Yet diffusion models that directly work on discrete data space do not fully exploit the power of iterative refinement, as the signals are lost during the transition between discrete states. Existing continuous diffusion models for discrete data have limited performance compared to discrete approaches, and the unclear link between them restricts the development of diffusion models for discrete data. In this work, we propose a continuous diffusion model for language modeling that incorporates the geometry of the underlying categorical distribution. We establish a connection between the discrete diffusion and continuous flow on the statistical manifold, and building on the analogy, we introduce a simple design for the diffusion process that generalizes previous discrete diffusion models. We further propose a simulation-free training framework based on radial symmetry and a simple technique to address the high dimensionality of the manifold. Comprehensive experiments on language modeling benchmarks and other modalities show that our method outperforms existing discrete diffusion models and approaches the performance of autoregressive models. Codes available at {\textbackslash}href\{https://github.com/harryjo97/{RDLM}\}\{https://github.com/harryjo97/{RDLM}\}.},
	number = {{arXiv}:2502.11564},
	publisher = {{arXiv}},
	author = {Jo, Jaehyeong and Hwang, Sung Ju},
	urldate = {2025-04-24},
	date = {2025-02-17},
	eprinttype = {arxiv},
	eprint = {2502.11564 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, {CT}-read, Application},
	file = {Full Text PDF:/Users/moonshine/Zotero/storage/698QNKYD/Jo and Hwang - 2025 - Continuous Diffusion Model for Language Modeling.pdf:application/pdf;Snapshot:/Users/moonshine/Zotero/storage/FNSKGKJW/2502.html:text/html},
}

@misc{chen_textdiffuser_2023,
	title = {{TextDiffuser}: Diffusion Models as Text Painters},
	url = {http://arxiv.org/abs/2305.10855},
	doi = {10.48550/arXiv.2305.10855},
	shorttitle = {{TextDiffuser}},
	abstract = {Diffusion models have gained increasing attention for their impressive generation abilities but currently struggle with rendering accurate and coherent text. To address this issue, we introduce {TextDiffuser}, focusing on generating images with visually appealing text that is coherent with backgrounds. {TextDiffuser} consists of two stages: first, a Transformer model generates the layout of keywords extracted from text prompts, and then diffusion models generate images conditioned on the text prompt and the generated layout. Additionally, we contribute the first large-scale text images dataset with {OCR} annotations, {MARIO}-10M, containing 10 million image-text pairs with text recognition, detection, and character-level segmentation annotations. We further collect the {MARIO}-Eval benchmark to serve as a comprehensive tool for evaluating text rendering quality. Through experiments and user studies, we show that {TextDiffuser} is flexible and controllable to create high-quality text images using text prompts alone or together with text template images, and conduct text inpainting to reconstruct incomplete images with text. The code, model, and dataset will be available at {\textbackslash}url\{https://aka.ms/textdiffuser\}.},
	number = {{arXiv}:2305.10855},
	publisher = {{arXiv}},
	author = {Chen, Jingye and Huang, Yupan and Lv, Tengchao and Cui, Lei and Chen, Qifeng and Wei, Furu},
	urldate = {2025-04-24},
	date = {2023-10-30},
	eprinttype = {arxiv},
	eprint = {2305.10855 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, {CT}-read, Application},
	file = {Full Text PDF:/Users/moonshine/Zotero/storage/K5DNIRQB/Chen et al. - 2023 - TextDiffuser Diffusion Models as Text Painters.pdf:application/pdf;Snapshot:/Users/moonshine/Zotero/storage/KRMWV4XJ/2305.html:text/html},
}

@misc{cardei_constrained_2025,
	title = {Constrained Language Generation with Discrete Diffusion Models},
	url = {http://arxiv.org/abs/2503.09790},
	doi = {10.48550/arXiv.2503.09790},
	abstract = {Constraints are critical in text generation as {LLM} outputs are often unreliable when it comes to ensuring generated outputs adhere to user defined instruction or general safety guidelines. To address this gap, we present Constrained Discrete Diffusion ({CDD}), a novel method for enforcing constraints on natural language by integrating discrete diffusion models with differentiable optimization. Unlike conventional text generators, which often rely on post-hoc filtering or model retraining for controllable generation, we propose imposing constraints directly into the discrete diffusion sampling process. We illustrate how this technique can be applied to satisfy a variety of natural language constraints, including (i) toxicity mitigation by preventing harmful content from emerging, (ii) character and sequence level lexical constraints, and (iii) novel molecule sequence generation with specific property adherence. Experimental results show that our constraint-aware procedure achieves high fidelity in meeting these requirements while preserving fluency and semantic coherence, outperforming auto-regressive and existing discrete diffusion approaches.},
	number = {{arXiv}:2503.09790},
	publisher = {{arXiv}},
	author = {Cardei, Michael and Christopher, Jacob K. and Hartvigsen, Thomas and Bartoldson, Brian R. and Kailkhura, Bhavya and Fioretto, Ferdinando},
	urldate = {2025-04-24},
	date = {2025-03-12},
	eprinttype = {arxiv},
	eprint = {2503.09790 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, {CT}-read, Application},
	file = {Full Text PDF:/Users/moonshine/Zotero/storage/AV2S772T/Cardei et al. - 2025 - Constrained Language Generation with Discrete Diffusion Models.pdf:application/pdf;Snapshot:/Users/moonshine/Zotero/storage/VZ4Y5PQE/2503.html:text/html},
}

@misc{chen_diffute_2023,
	title = {{DiffUTE}: Universal Text Editing Diffusion Model},
	url = {http://arxiv.org/abs/2305.10825},
	doi = {10.48550/arXiv.2305.10825},
	shorttitle = {{DiffUTE}},
	abstract = {Diffusion model based language-guided image editing has achieved great success recently. However, existing state-of-the-art diffusion models struggle with rendering correct text and text style during generation. To tackle this problem, we propose a universal self-supervised text editing diffusion model ({DiffUTE}), which aims to replace or modify words in the source image with another one while maintaining its realistic appearance. Specifically, we build our model on a diffusion model and carefully modify the network structure to enable the model for drawing multilingual characters with the help of glyph and position information. Moreover, we design a self-supervised learning framework to leverage large amounts of web data to improve the representation ability of the model. Experimental results show that our method achieves an impressive performance and enables controllable editing on in-the-wild images with high fidelity. Our code will be avaliable in {\textbackslash}url\{https://github.com/chenhaoxing/{DiffUTE}\}.},
	number = {{arXiv}:2305.10825},
	publisher = {{arXiv}},
	author = {Chen, Haoxing and Xu, Zhuoer and Gu, Zhangxuan and Lan, Jun and Zheng, Xing and Li, Yaohui and Meng, Changhua and Zhu, Huijia and Wang, Weiqiang},
	urldate = {2025-04-24},
	date = {2023-10-18},
	eprinttype = {arxiv},
	eprint = {2305.10825 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, Application},
	file = {Full Text PDF:/Users/moonshine/Zotero/storage/GZLLE3R2/Chen et al. - 2023 - DiffUTE Universal Text Editing Diffusion Model.pdf:application/pdf;Snapshot:/Users/moonshine/Zotero/storage/KJEDVKGK/2305.html:text/html},
}

@misc{christopher_speculative_2025,
	title = {Speculative Diffusion Decoding: Accelerating Language Generation through Diffusion},
	url = {http://arxiv.org/abs/2408.05636},
	doi = {10.48550/arXiv.2408.05636},
	shorttitle = {Speculative Diffusion Decoding},
	abstract = {Speculative decoding has emerged as a widely adopted method to accelerate large language model inference without sacrificing the quality of the model outputs. While this technique has facilitated notable speed improvements by enabling parallel sequence verification, its efficiency remains inherently limited by the reliance on incremental token generation in existing draft models. To overcome this limitation, this paper proposes an adaptation of speculative decoding which uses discrete diffusion models to generate draft sequences. This allows parallelization of both the drafting and verification steps, providing significant speedups to the inference process. Our proposed approach, \${\textbackslash}textit\{Speculative Diffusion Decoding ({SpecDiff})\}\$, is validated on standard language generation benchmarks and empirically demonstrated to provide up to 7.2x speedups over standard generation processes and up to 1.75x speedups over existing speculative decoding approaches.},
	number = {{arXiv}:2408.05636},
	publisher = {{arXiv}},
	author = {Christopher, Jacob K. and Bartoldson, Brian R. and Ben-Nun, Tal and Cardei, Michael and Kailkhura, Bhavya and Fioretto, Ferdinando},
	urldate = {2025-04-24},
	date = {2025-02-10},
	eprinttype = {arxiv},
	eprint = {2408.05636 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, {CT}-read, {InferenceSpeed}, Collaboration},
	file = {Full Text PDF:/Users/moonshine/Zotero/storage/ULFDJMSC/Christopher et al. - 2025 - Speculative Diffusion Decoding Accelerating Language Generation through Diffusion.pdf:application/pdf;Snapshot:/Users/moonshine/Zotero/storage/3C67SYHH/2408.html:text/html},
}

@misc{krojer_are_2023,
	title = {Are Diffusion Models Vision-And-Language Reasoners?},
	url = {http://arxiv.org/abs/2305.16397},
	doi = {10.48550/arXiv.2305.16397},
	abstract = {Text-conditioned image generation models have recently shown immense qualitative success using denoising diffusion processes. However, unlike discriminative vision-and-language models, it is a non-trivial task to subject these diffusion-based generative models to automatic fine-grained quantitative evaluation of high-level phenomena such as compositionality. Towards this goal, we perform two innovations. First, we transform diffusion-based models (in our case, Stable Diffusion) for any image-text matching ({ITM}) task using a novel method called {DiffusionITM}. Second, we introduce the Generative-Discriminative Evaluation Benchmark ({GDBench}) benchmark with 7 complex vision-and-language tasks, bias evaluation and detailed analysis. We find that Stable Diffusion + {DiffusionITM} is competitive on many tasks and outperforms {CLIP} on compositional tasks like like {CLEVR} and Winoground. We further boost its compositional performance with a transfer setup by fine-tuning on {MS}-{COCO} while retaining generative capabilities. We also measure the stereotypical bias in diffusion models, and find that Stable Diffusion 2.1 is, for the most part, less biased than Stable Diffusion 1.5. Overall, our results point in an exciting direction bringing discriminative and generative model evaluation closer. We will release code and benchmark setup soon.},
	number = {{arXiv}:2305.16397},
	publisher = {{arXiv}},
	author = {Krojer, Benno and Poole-Dayan, Elinor and Voleti, Vikram and Pal, Christopher and Reddy, Siva},
	urldate = {2025-04-24},
	date = {2023-11-02},
	eprinttype = {arxiv},
	eprint = {2305.16397 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Computation and Language, {CT}-read, Collaboration},
	file = {Full Text PDF:/Users/moonshine/Zotero/storage/TAHLND5Q/Krojer et al. - 2023 - Are Diffusion Models Vision-And-Language Reasoners.pdf:application/pdf;Snapshot:/Users/moonshine/Zotero/storage/EUA9NGNE/2305.html:text/html},
}

@misc{wallace_diffusion_2023,
	title = {Diffusion Model Alignment Using Direct Preference Optimization},
	url = {http://arxiv.org/abs/2311.12908},
	doi = {10.48550/arXiv.2311.12908},
	abstract = {Large language models ({LLMs}) are fine-tuned using human comparison data with Reinforcement Learning from Human Feedback ({RLHF}) methods to make them better aligned with users' preferences. In contrast to {LLMs}, human preference learning has not been widely explored in text-to-image diffusion models; the best existing approach is to fine-tune a pretrained model using carefully curated high quality images and captions to improve visual appeal and text alignment. We propose Diffusion-{DPO}, a method to align diffusion models to human preferences by directly optimizing on human comparison data. Diffusion-{DPO} is adapted from the recently developed Direct Preference Optimization ({DPO}), a simpler alternative to {RLHF} which directly optimizes a policy that best satisfies human preferences under a classification objective. We re-formulate {DPO} to account for a diffusion model notion of likelihood, utilizing the evidence lower bound to derive a differentiable objective. Using the Pick-a-Pic dataset of 851K crowdsourced pairwise preferences, we fine-tune the base model of the state-of-the-art Stable Diffusion {XL} ({SDXL})-1.0 model with Diffusion-{DPO}. Our fine-tuned base model significantly outperforms both base {SDXL}-1.0 and the larger {SDXL}-1.0 model consisting of an additional refinement model in human evaluation, improving visual appeal and prompt alignment. We also develop a variant that uses {AI} feedback and has comparable performance to training on human preferences, opening the door for scaling of diffusion model alignment methods.},
	number = {{arXiv}:2311.12908},
	publisher = {{arXiv}},
	author = {Wallace, Bram and Dang, Meihua and Rafailov, Rafael and Zhou, Linqi and Lou, Aaron and Purushwalkam, Senthil and Ermon, Stefano and Xiong, Caiming and Joty, Shafiq and Naik, Nikhil},
	urldate = {2025-04-24},
	date = {2023-11-21},
	eprinttype = {arxiv},
	eprint = {2311.12908 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, {CT}-read},
	file = {Full Text PDF:/Users/moonshine/Zotero/storage/JBZQIMPK/Wallace et al. - 2023 - Diffusion Model Alignment Using Direct Preference Optimization.pdf:application/pdf;Snapshot:/Users/moonshine/Zotero/storage/B8PK9GRI/2311.html:text/html},
}

@misc{li_ddpt_2025,
	title = {{DDPT}: Diffusion-Driven Prompt Tuning for Large Language Model Code Generation},
	url = {http://arxiv.org/abs/2504.04351},
	doi = {10.48550/arXiv.2504.04351},
	shorttitle = {{DDPT}},
	abstract = {Large Language Models ({LLMs}) have demonstrated remarkable capabilities in code generation. However, the quality of the generated code is heavily dependent on the structure and composition of the prompts used. Crafting high-quality prompts is a challenging task that requires significant knowledge and skills of prompt engineering. To advance the automation support for the prompt engineering for {LLM}-based code generation, we propose a novel solution Diffusion-Driven Prompt Tuning ({DDPT}) that learns how to generate optimal prompt embedding from Gaussian Noise to automate the prompt engineering for code generation. We evaluate the feasibility of diffusion-based optimization and abstract the optimal prompt embedding as a directional vector toward the optimal embedding. We use the code generation loss given by the {LLMs} to help the diffusion model capture the distribution of optimal prompt embedding during training. The trained diffusion model can build a path from the noise distribution to the optimal distribution at the sampling phrase, the evaluation result demonstrates that {DDPT} helps improve the prompt optimization for code generation.},
	number = {{arXiv}:2504.04351},
	publisher = {{arXiv}},
	author = {Li, Jinyang and Hyun, Sangwon and Babar, M. Ali},
	urldate = {2025-04-24},
	date = {2025-04-06},
	eprinttype = {arxiv},
	eprint = {2504.04351 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Software Engineering, {CT}-read, Collaboration, Application, {PromptEngineering}},
	file = {Full Text PDF:/Users/moonshine/Zotero/storage/2RK39ST4/Li et al. - 2025 - DDPT Diffusion-Driven Prompt Tuning for Large Language Model Code Generation.pdf:application/pdf;Snapshot:/Users/moonshine/Zotero/storage/8LX4REXE/2504.html:text/html},
}

@misc{cao_survey_2023,
	title = {A Survey on Generative Diffusion Model},
	url = {http://arxiv.org/abs/2209.02646},
	doi = {10.48550/arXiv.2209.02646},
	abstract = {Deep generative models have unlocked another profound realm of human creativity. By capturing and generalizing patterns within data, we have entered the epoch of all-encompassing Artificial Intelligence for General Creativity ({AIGC}). Notably, diffusion models, recognized as one of the paramount generative models, materialize human ideation into tangible instances across diverse domains, encompassing imagery, text, speech, biology, and healthcare. To provide advanced and comprehensive insights into diffusion, this survey comprehensively elucidates its developmental trajectory and future directions from three distinct angles: the fundamental formulation of diffusion, algorithmic enhancements, and the manifold applications of diffusion. Each layer is meticulously explored to offer a profound comprehension of its evolution. Structured and summarized approaches are presented in https://github.com/chq1155/A-Survey-on-Generative-Diffusion-Model.},
	number = {{arXiv}:2209.02646},
	publisher = {{arXiv}},
	author = {Cao, Hanqun and Tan, Cheng and Gao, Zhangyang and Xu, Yilun and Chen, Guangyong and Heng, Pheng-Ann and Li, Stan Z.},
	urldate = {2025-04-24},
	date = {2023-12-23},
	eprinttype = {arxiv},
	eprint = {2209.02646 [cs]},
	keywords = {Literature Review, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Graphics, {CT}-read, Application},
	file = {Full Text PDF:/Users/moonshine/Zotero/storage/HIUSF95L/Cao et al. - 2023 - A Survey on Generative Diffusion Model.pdf:application/pdf;Snapshot:/Users/moonshine/Zotero/storage/EW274J2H/2209.html:text/html},
}

@misc{venkatraman_amortizing_2025,
	title = {Amortizing intractable inference in diffusion models for vision, language, and control},
	url = {http://arxiv.org/abs/2405.20971},
	doi = {10.48550/arXiv.2405.20971},
	abstract = {Diffusion models have emerged as effective distribution estimators in vision, language, and reinforcement learning, but their use as priors in downstream tasks poses an intractable posterior inference problem. This paper studies amortized sampling of the posterior over data, \${\textbackslash}mathbf\{x\}{\textbackslash}sim p{\textasciicircum}\{{\textbackslash}rm post\}({\textbackslash}mathbf\{x\}){\textbackslash}propto p({\textbackslash}mathbf\{x\})r({\textbackslash}mathbf\{x\})\$, in a model that consists of a diffusion generative model prior \$p({\textbackslash}mathbf\{x\})\$ and a black-box constraint or likelihood function \$r({\textbackslash}mathbf\{x\})\$. We state and prove the asymptotic correctness of a data-free learning objective, relative trajectory balance, for training a diffusion model that samples from this posterior, a problem that existing methods solve only approximately or in restricted cases. Relative trajectory balance arises from the generative flow network perspective on diffusion models, which allows the use of deep reinforcement learning techniques to improve mode coverage. Experiments illustrate the broad potential of unbiased inference of arbitrary posteriors under diffusion priors: in vision (classifier guidance), language (infilling under a discrete diffusion {LLM}), and multimodal data (text-to-image generation). Beyond generative modeling, we apply relative trajectory balance to the problem of continuous control with a score-based behavior prior, achieving state-of-the-art results on benchmarks in offline reinforcement learning.},
	number = {{arXiv}:2405.20971},
	publisher = {{arXiv}},
	author = {Venkatraman, Siddarth and Jain, Moksh and Scimeca, Luca and Kim, Minsu and Sendera, Marcin and Hasan, Mohsin and Rowe, Luke and Mittal, Sarthak and Lemos, Pablo and Bengio, Emmanuel and Adam, Alexandre and Rector-Brooks, Jarrid and Bengio, Yoshua and Berseth, Glen and Malkin, Nikolay},
	urldate = {2025-04-24},
	date = {2025-01-13},
	eprinttype = {arxiv},
	eprint = {2405.20971 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Computation and Language, Computer Science - Graphics, Multi-modal, {CT}-read},
	file = {Full Text PDF:/Users/moonshine/Zotero/storage/R2T9S853/Venkatraman et al. - 2025 - Amortizing intractable inference in diffusion models for vision, language, and control.pdf:application/pdf;Snapshot:/Users/moonshine/Zotero/storage/RQR6K2GJ/2405.html:text/html},
}

@misc{chen_diffpo_2025,
	title = {{DiffPO}: Diffusion-styled Preference Optimization for Efficient Inference-Time Alignment of Large Language Models},
	url = {http://arxiv.org/abs/2503.04240},
	doi = {10.48550/arXiv.2503.04240},
	shorttitle = {{DiffPO}},
	abstract = {Inference-time alignment provides an efficient alternative for aligning {LLMs} with humans. However, these approaches still face challenges, such as limited scalability due to policy-specific value functions and latency during the inference phase. In this paper, we propose a novel approach, Diffusion-styled Preference Optimization ({\textbackslash}model), which provides an efficient and policy-agnostic solution for aligning {LLMs} with humans. By directly performing alignment at sentence level, {\textbackslash}model{\textasciitilde}avoids the time latency associated with token-level generation. Designed as a plug-and-play module, {\textbackslash}model{\textasciitilde}can be seamlessly integrated with various base models to enhance their alignment. Extensive experiments on {AlpacaEval} 2, {MT}-bench, and {HH}-{RLHF} demonstrate that {\textbackslash}model{\textasciitilde}achieves superior alignment performance across various settings, achieving a favorable trade-off between alignment quality and inference-time latency. Furthermore, {\textbackslash}model{\textasciitilde}demonstrates model-agnostic scalability, significantly improving the performance of large models such as Llama-3-70B.},
	number = {{arXiv}:2503.04240},
	publisher = {{arXiv}},
	author = {Chen, Ruizhe and Chai, Wenhao and Yang, Zhifei and Zhang, Xiaotian and Zhou, Joey Tianyi and Quek, Tony and Poria, Soujanya and Liu, Zuozhu},
	urldate = {2025-04-24},
	date = {2025-03-09},
	eprinttype = {arxiv},
	eprint = {2503.04240 [cs]},
	keywords = {Computer Science - Computation and Language, {CT}-read, Collaboration, Application},
	file = {Full Text PDF:/Users/moonshine/Zotero/storage/QULLVYLA/Chen et al. - 2025 - DiffPO Diffusion-styled Preference Optimization for Efficient Inference-Time Alignment of Large Lan.pdf:application/pdf;Snapshot:/Users/moonshine/Zotero/storage/S6D26Q36/2503.html:text/html},
}

@article{xu_diff-zsvqa_2025,
	title = {Diff-{ZsVQA}: Zero-shot Visual Question Answering with Frozen Large Language Models Using Diffusion Model},
	volume = {275},
	issn = {0957-4174},
	url = {https://www.sciencedirect.com/science/article/pii/S0957417425005731},
	doi = {10.1016/j.eswa.2025.126951},
	shorttitle = {Diff-{ZsVQA}},
	abstract = {Visual Question Answering ({VQA}) methods leveraging Large Language Models ({LLMs}) aim to enhance performance in few/zero-shot scenarios. While the results attained by these approaches were outstanding, there remains scope for further enhancement. Given the remarkable capabilities demonstrated by Diffusion Models ({DMs}), we recognize that the {DMs} can potentially improve the performance of {VQA} by optimizing the generation of captions. Furthermore, existing approaches for prompt construction neglect the influence of non-original questions and generated question-answer ({QA}) pairs, which leads to adverse effects on the inference. This paper proposes a novel framework called Diffused Zero-shot {VQA}, shortly Diff-{ZsVQA}, which innovatively incorporates a powerful {DM} into the {LLM}-based {VQA} pipeline for image-to-text converting. Moreover, to reduce the impact of non-original questions and generated {QA} pairs, we devise an Original-Question-Centric ({OQC}) prompt whose examples’ questions are identical while contexts are diverse. We first construct initial prompts to formulate answer candidates, then the final answer is selected among options in answer heuristics via {OQC} prompting. Compared with previous {LLM}-based {VQA} methods, the proposed architecture is simpler and it brings a higher efficiency to predictions in zero-shot {VQA}. Extensive experiments demonstrate that Diff-{ZsVQA} with {OQC} prompt achieves competitive performance with higher inference speed than most existing methods.},
	pages = {126951},
	journaltitle = {Expert Systems with Applications},
	shortjournal = {Expert Systems with Applications},
	author = {Xu, Quanxing and Li, Jian and Tian, Yuhao and Zhou, Ling and Zhang, Feifei and Huang, Rubing},
	urldate = {2025-04-24},
	date = {2025-05-25},
	keywords = {Diffusion models, Large language models, Multi-modal, Prompt, Visual question answering, Zero-shot, {CT}-read, Collaboration, Application, {PromptEngineering}},
	file = {1-s2.0-S0957417425005731-main.pdf:/Users/moonshine/Zotero/storage/QJTCU6DN/1-s2.0-S0957417425005731-main.pdf:application/pdf},
}

@misc{liu_p3sum_2024,
	title = {P{\textasciicircum}3SUM: Preserving Author's Perspective in News Summarization with Diffusion Language Models},
	url = {http://arxiv.org/abs/2311.09741},
	doi = {10.48550/arXiv.2311.09741},
	shorttitle = {P{\textasciicircum}3SUM},
	abstract = {In this work, we take a first step towards designing summarization systems that are faithful to the author's intent, not only the semantic content of the article. Focusing on a case study of preserving political perspectives in news summarization, we find that existing approaches alter the political opinions and stances of news articles in more than 50\% of summaries, misrepresenting the intent and perspectives of the news authors. We thus propose P{\textasciicircum}3SUM, a diffusion model-based summarization approach controlled by political perspective classifiers. In P{\textasciicircum}3SUM, the political leaning of a generated summary is iteratively evaluated at each decoding step, and any drift from the article's original stance incurs a loss back-propagated to the embedding layers, steering the political stance of the summary at inference time. Extensive experiments on three news summarization datasets demonstrate that P{\textasciicircum}3SUM outperforms state-of-the-art summarization systems and large language models by up to 13.7\% in terms of the success rate of stance preservation, with competitive performance on standard metrics of summarization quality. Our findings present a first analysis of preservation of pragmatic features in summarization, highlight the lacunae in existing summarization models -- that even state-of-the-art models often struggle to preserve author's intents -- and develop new summarization systems that are more faithful to author's perspectives.},
	number = {{arXiv}:2311.09741},
	publisher = {{arXiv}},
	author = {Liu, Yuhan and Feng, Shangbin and Han, Xiaochuang and Balachandran, Vidhisha and Park, Chan Young and Kumar, Sachin and Tsvetkov, Yulia},
	urldate = {2025-04-24},
	date = {2024-04-04},
	eprinttype = {arxiv},
	eprint = {2311.09741 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, {CT}-read, Application},
	file = {Full Text PDF:/Users/moonshine/Zotero/storage/M692AYJE/Liu et al. - 2024 - P^3SUM Preserving Author's Perspective in News Summarization with Diffusion Language Models.pdf:application/pdf;Snapshot:/Users/moonshine/Zotero/storage/I5X7QSPD/2311.html:text/html},
}

@misc{tae_tess_2025,
	title = {{TESS} 2: A Large-Scale Generalist Diffusion Language Model},
	url = {http://arxiv.org/abs/2502.13917},
	doi = {10.48550/arXiv.2502.13917},
	shorttitle = {{TESS} 2},
	abstract = {We introduce {TESS} 2, a general instruction-following diffusion language model that outperforms contemporary instruction-tuned diffusion models, as well as matches and sometimes exceeds strong autoregressive ({AR}) models. We train {TESS} 2 by first adapting a strong {AR} model via continued pretraining with the usual cross-entropy as diffusion loss, and then performing further instruction tuning. We find that adaptation training as well as the choice of the base model is crucial for training good instruction-following diffusion models. We further propose reward guidance, a novel and modular inference-time guidance procedure to align model outputs without needing to train the underlying model. Finally, we show that {TESS} 2 further improves with increased inference-time compute, highlighting the utility of diffusion {LMs} in having fine-grained controllability over the amount of compute used at inference time. Code and models are available at https://github.com/hamishivi/tess-2.},
	number = {{arXiv}:2502.13917},
	publisher = {{arXiv}},
	author = {Tae, Jaesung and Ivison, Hamish and Kumar, Sachin and Cohan, Arman},
	urldate = {2025-04-24},
	date = {2025-02-19},
	eprinttype = {arxiv},
	eprint = {2502.13917 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, Application},
	file = {Full Text PDF:/Users/moonshine/Zotero/storage/T2C368ER/Tae et al. - 2025 - TESS 2 A Large-Scale Generalist Diffusion Language Model.pdf:application/pdf;Snapshot:/Users/moonshine/Zotero/storage/PXUUZZ7V/2502.html:text/html},
}

@misc{he_text-driven_2025,
	title = {Text-Driven Diffusion Model for Sign Language Production},
	url = {http://arxiv.org/abs/2503.15914},
	doi = {10.48550/arXiv.2503.15914},
	abstract = {We introduce the hfut-lmc team's solution to the {SLRTP} Sign Production Challenge. The challenge aims to generate semantically aligned sign language pose sequences from text inputs. To this end, we propose a Text-driven Diffusion Model ({TDM}) framework. During the training phase, {TDM} utilizes an encoder to encode text sequences and incorporates them into the diffusion model as conditional input to generate sign pose sequences. To guarantee the high quality and accuracy of the generated pose sequences, we utilize two key loss functions. The joint loss function L\_\{joint\} is used to precisely measure and minimize the differences between the joint positions of the generated pose sequences and those of the ground truth. Similarly, the bone orientation loss function L\_\{bone\} is instrumental in ensuring that the orientation of the bones in the generated poses aligns with the actual, correct orientations. In the inference stage, the {TDM} framework takes on a different yet equally important task. It starts with noisy sequences and, under the strict constraints of the text conditions, gradually refines and generates semantically consistent sign language pose sequences. Our carefully designed framework performs well on the sign language production task, and our solution achieves a {BLEU}-1 score of 20.17, placing second in the challenge.},
	number = {{arXiv}:2503.15914},
	publisher = {{arXiv}},
	author = {He, Jiayi and Wang, Xu and Zhang, Ruobei and Tang, Shengeng and Wang, Yaxiong and Cheng, Lechao},
	urldate = {2025-04-24},
	date = {2025-03-20},
	eprinttype = {arxiv},
	eprint = {2503.15914 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, {CT}-read, Application},
	file = {Full Text PDF:/Users/moonshine/Zotero/storage/QA3L79I6/He et al. - 2025 - Text-Driven Diffusion Model for Sign Language Production.pdf:application/pdf;Snapshot:/Users/moonshine/Zotero/storage/WCIAJCND/2503.html:text/html},
}

@misc{arriola_block_2025,
	title = {Block Diffusion: Interpolating Between Autoregressive and Diffusion Language Models},
	url = {http://arxiv.org/abs/2503.09573},
	doi = {10.48550/arXiv.2503.09573},
	shorttitle = {Block Diffusion},
	abstract = {Diffusion language models offer unique benefits over autoregressive models due to their potential for parallelized generation and controllability, yet they lag in likelihood modeling and are limited to fixed-length generation. In this work, we introduce a class of block diffusion language models that interpolate between discrete denoising diffusion and autoregressive models. Block diffusion overcomes key limitations of both approaches by supporting flexible-length generation and improving inference efficiency with {KV} caching and parallel token sampling. We propose a recipe for building effective block diffusion models that includes an efficient training algorithm, estimators of gradient variance, and data-driven noise schedules to minimize the variance. Block diffusion sets a new state-of-the-art performance among diffusion models on language modeling benchmarks and enables generation of arbitrary-length sequences. We provide the code, along with the model weights and blog post on the project page: https://m-arriola.com/bd3lms/},
	number = {{arXiv}:2503.09573},
	publisher = {{arXiv}},
	author = {Arriola, Marianne and Gokaslan, Aaron and Chiu, Justin T. and Yang, Zhihan and Qi, Zhixuan and Han, Jiaqi and Sahoo, Subham Sekhar and Kuleshov, Volodymyr},
	urldate = {2025-04-24},
	date = {2025-03-18},
	eprinttype = {arxiv},
	eprint = {2503.09573 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Value, {CT}-read, Interporablity, {InferenceSpeed}, Collaboration, {ICLR}2025},
	file = {Full Text PDF:/Users/moonshine/Zotero/storage/4DJ4F9ZC/Arriola et al. - 2025 - Block Diffusion Interpolating Between Autoregressive and Diffusion Language Models.pdf:application/pdf;Snapshot:/Users/moonshine/Zotero/storage/22266RHJ/2503.html:text/html},
}

@misc{liu_hybridvla_2025,
	title = {{HybridVLA}: Collaborative Diffusion and Autoregression in a Unified Vision-Language-Action Model},
	url = {http://arxiv.org/abs/2503.10631},
	doi = {10.48550/arXiv.2503.10631},
	shorttitle = {{HybridVLA}},
	abstract = {Recent advancements in vision-language models ({VLMs}) for common-sense reasoning have led to the development of vision-language-action ({VLA}) models, enabling robots to perform generalized manipulation. Although existing autoregressive {VLA} methods leverage large-scale pretrained knowledge, they disrupt the continuity of actions. Meanwhile, some {VLA} methods incorporate an additional diffusion head to predict continuous actions, relying solely on {VLM}-extracted features, which limits their reasoning capabilities. In this paper, we introduce {HybridVLA}, a unified framework that seamlessly integrates the strengths of both autoregressive and diffusion policies within a single large language model, rather than simply connecting them. To bridge the generation gap, a collaborative training recipe is proposed that injects the diffusion modeling directly into the next-token prediction. With this recipe, we find that these two forms of action prediction not only reinforce each other but also exhibit varying performance across different tasks. Therefore, we design a collaborative action ensemble mechanism that adaptively fuses these two predictions, leading to more robust control. In experiments, {HybridVLA} outperforms previous state-of-the-art {VLA} methods across various simulation and real-world tasks, including both single-arm and dual-arm robots, while demonstrating stable manipulation in previously unseen configurations.},
	number = {{arXiv}:2503.10631},
	publisher = {{arXiv}},
	author = {Liu, Jiaming and Chen, Hao and An, Pengju and Liu, Zhuoyang and Zhang, Renrui and Gu, Chenyang and Li, Xiaoqi and Guo, Ziyu and Chen, Sixiang and Liu, Mengzhen and Hou, Chengkai and Zhao, Mengdi and Zhou, {KC} alex and Heng, Pheng-Ann and Zhang, Shanghang},
	urldate = {2025-04-24},
	date = {2025-03-17},
	eprinttype = {arxiv},
	eprint = {2503.10631 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Multi-modal, Computer Science - Robotics, Robotic, {CT}-read, Collaboration, Application},
	file = {Full Text PDF:/Users/moonshine/Zotero/storage/HY4GKSXZ/Liu et al. - 2025 - HybridVLA Collaborative Diffusion and Autoregression in a Unified Vision-Language-Action Model.pdf:application/pdf;Snapshot:/Users/moonshine/Zotero/storage/VFK8IJGI/2503.html:text/html},
}

@misc{cetin_large_2025,
	title = {Large Language Models to Diffusion Finetuning},
	url = {http://arxiv.org/abs/2501.15781},
	doi = {10.48550/arXiv.2501.15781},
	abstract = {We propose a new finetuning method to provide pre-trained large language models ({LMs}) the ability to scale test-time compute through the diffusion framework. By increasing the number of diffusion steps, we show our finetuned models achieve monotonically increasing accuracy, directly translating to improved performance across downstream tasks. Furthermore, our finetuned models can expertly answer questions on specific topics by integrating powerful guidance techniques, and autonomously determine the compute required for a given problem by leveraging adaptive {ODE} solvers. Our method is universally applicable to any foundation model pre-trained with a cross-entropy loss and does not modify any of its original weights, fully preserving its strong single-step generation capabilities. We show our method is more effective and fully compatible with traditional finetuning approaches, introducing an orthogonal new direction to unify the strengths of the autoregressive and diffusion frameworks.},
	number = {{arXiv}:2501.15781},
	publisher = {{arXiv}},
	author = {Cetin, Edoardo and Zhao, Tianyu and Tang, Yujin},
	urldate = {2025-04-24},
	date = {2025-01-27},
	eprinttype = {arxiv},
	eprint = {2501.15781 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, {CT}-read},
	file = {Full Text PDF:/Users/moonshine/Zotero/storage/IVB3AFMF/Cetin et al. - 2025 - Large Language Models to Diffusion Finetuning.pdf:application/pdf;Snapshot:/Users/moonshine/Zotero/storage/LFBJ7K8X/2501.html:text/html},
}

@misc{bortoli_accelerated_2025,
	title = {Accelerated Diffusion Models via Speculative Sampling},
	url = {http://arxiv.org/abs/2501.05370},
	doi = {10.48550/arXiv.2501.05370},
	abstract = {Speculative sampling is a popular technique for accelerating inference in Large Language Models by generating candidate tokens using a fast draft model and accepting or rejecting them based on the target model's distribution. While speculative sampling was previously limited to discrete sequences, we extend it to diffusion models, which generate samples via continuous, vector-valued Markov chains. In this context, the target model is a high-quality but computationally expensive diffusion model. We propose various drafting strategies, including a simple and effective approach that does not require training a draft model and is applicable out of the box to any diffusion model. Our experiments demonstrate significant generation speedup on various diffusion models, halving the number of function evaluations, while generating exact samples from the target model.},
	number = {{arXiv}:2501.05370},
	publisher = {{arXiv}},
	author = {Bortoli, Valentin De and Galashov, Alexandre and Gretton, Arthur and Doucet, Arnaud},
	urldate = {2025-04-24},
	date = {2025-01-09},
	eprinttype = {arxiv},
	eprint = {2501.05370 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, {CT}-read, {InferenceSpeed}, Theory},
	file = {Full Text PDF:/Users/moonshine/Zotero/storage/G89AYJY6/Bortoli et al. - 2025 - Accelerated Diffusion Models via Speculative Sampling.pdf:application/pdf;Snapshot:/Users/moonshine/Zotero/storage/TI3QQXQS/2501.html:text/html},
}

@misc{ye_diffusion_2025,
	title = {{DIFFUSION} {LANGUAGE} {MODELS} {CAN} {PERFORM} {MANY}  {TASKS} {WITH} {SCALING} {AND} {INSTRUCTION}-{FINETUNING}},
	url = {http://arxiv.org/abs/2308.12219},
	doi = {10.48550/arXiv.2308.12219},
	abstract = {The recent surge of generative {AI} has been fueled by the generative power of diffusion probabilistic models and the scalable capabilities of large language models. Despite their potential, it remains elusive whether diffusion language models can solve general language tasks comparable to their autoregressive counterparts. This paper demonstrates that scaling diffusion models w.r.t. data, sizes, and tasks can effectively make them strong language learners. We build competent diffusion language models at scale by first acquiring knowledge from massive data via masked language modeling pretraining thanks to their intrinsic connections. We then reprogram pretrained masked language models into diffusion language models via diffusive adaptation, wherein task-specific finetuning and instruction finetuning are explored to unlock their versatility in solving general language tasks. Experiments show that scaling diffusion language models consistently improves performance across downstream language tasks. We further discover that instruction finetuning can elicit zero-shot and few-shot in-context learning abilities that help tackle many unseen tasks by following natural language instructions, and show promise in advanced and challenging abilities such as reasoning.},
	number = {{arXiv}:2308.12219},
	publisher = {{arXiv}},
	author = {Ye, Jiasheng and Zheng, Zaixiang and Bao, Yu and Qian, Lihua and Gu, Quanquan},
	urldate = {2025-04-24},
	date = {2025-02-24},
	eprinttype = {arxiv},
	eprint = {2308.12219 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Value, Multi-modal, {CT}-read, Reasoning},
	file = {2308.12219v3.pdf:/Users/moonshine/Zotero/storage/Q5F4QQKW/2308.12219v3.pdf:application/pdf;Snapshot:/Users/moonshine/Zotero/storage/UAYN3QVX/2308.html:text/html},
}

@misc{zou_survey_2023,
	title = {A Survey of Diffusion Models in Natural Language Processing},
	url = {http://arxiv.org/abs/2305.14671},
	doi = {10.48550/arXiv.2305.14671},
	abstract = {This survey paper provides a comprehensive review of the use of diffusion models in natural language processing ({NLP}). Diffusion models are a class of mathematical models that aim to capture the diffusion of information or signals across a network or manifold. In {NLP}, diffusion models have been used in a variety of applications, such as natural language generation, sentiment analysis, topic modeling, and machine translation. This paper discusses the different formulations of diffusion models used in {NLP}, their strengths and limitations, and their applications. We also perform a thorough comparison between diffusion models and alternative generative models, specifically highlighting the autoregressive ({AR}) models, while also examining how diverse architectures incorporate the Transformer in conjunction with diffusion models. Compared to {AR} models, diffusion models have significant advantages for parallel generation, text interpolation, token-level controls such as syntactic structures and semantic contents, and robustness. Exploring further permutations of integrating Transformers into diffusion models would be a valuable pursuit. Also, the development of multimodal diffusion models and large-scale diffusion language models with notable capabilities for few-shot learning would be important directions for the future advance of diffusion models in {NLP}.},
	number = {{arXiv}:2305.14671},
	publisher = {{arXiv}},
	author = {Zou, Hao and Kim, Zae Myung and Kang, Dongyeop},
	urldate = {2025-04-24},
	date = {2023-06-14},
	langid = {american},
	eprinttype = {arxiv},
	eprint = {2305.14671 [cs]},
	keywords = {Literature Review, Computer Science - Computation and Language, {CT}-read, Collaboration, Application},
	file = {Full Text PDF:/Users/moonshine/Zotero/storage/FU9RR6RZ/Zou 等 - 2023 - A Survey of Diffusion Models in Natural Language Processing.pdf:application/pdf;Snapshot:/Users/moonshine/Zotero/storage/LB3QY2QG/2305.html:text/html},
}

@misc{shi_simplified_2025,
	title = {Simplified and Generalized Masked Diffusion for Discrete Data},
	url = {http://arxiv.org/abs/2406.04329},
	doi = {10.48550/arXiv.2406.04329},
	abstract = {Masked (or absorbing) diffusion is actively explored as an alternative to autoregressive models for generative modeling of discrete data. However, existing work in this area has been hindered by unnecessarily complex model formulations and unclear relationships between different perspectives, leading to suboptimal parameterization, training objectives, and ad hoc adjustments to counteract these issues. In this work, we aim to provide a simple and general framework that unlocks the full potential of masked diffusion models. We show that the continuous-time variational objective of masked diffusion models is a simple weighted integral of cross-entropy losses. Our framework also enables training generalized masked diffusion models with state-dependent masking schedules. When evaluated by perplexity, our models trained on {OpenWebText} surpass prior diffusion language models at {GPT}-2 scale and demonstrate superior performance on 4 out of 5 zero-shot language modeling tasks. Furthermore, our models vastly outperform previous discrete diffusion models on pixel-level image modeling, achieving 2.75 ({CIFAR}-10) and 3.40 ({ImageNet} 64x64) bits per dimension that are better than autoregressive models of similar sizes. Our code is available at https://github.com/google-deepmind/md4.},
	number = {{arXiv}:2406.04329},
	publisher = {{arXiv}},
	author = {Shi, Jiaxin and Han, Kehang and Wang, Zhe and Doucet, Arnaud and Titsias, Michalis K.},
	urldate = {2025-04-26},
	date = {2025-01-16},
	eprinttype = {arxiv},
	eprint = {2406.04329 [cs]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Theory},
	file = {Full Text PDF:/Users/moonshine/Zotero/storage/LC3EFSET/Shi 等 - 2025 - Simplified and Generalized Masked Diffusion for Discrete Data.pdf:application/pdf;Snapshot:/Users/moonshine/Zotero/storage/UDGA454C/2406.html:text/html},
}

@misc{gao_diffsds_2023,
	title = {{DiffSDS}: A language diffusion model for protein backbone inpainting under geometric conditions and constraints},
	url = {http://arxiv.org/abs/2301.09642},
	doi = {10.48550/arXiv.2301.09642},
	shorttitle = {{DiffSDS}},
	abstract = {Have you ever been troubled by the complexity and computational cost of {SE}(3) protein structure modeling and been amazed by the simplicity and power of language modeling? Recent work has shown promise in simplifying protein structures as sequences of protein angles; therefore, language models could be used for unconstrained protein backbone generation. Unfortunately, such simplification is unsuitable for the constrained protein inpainting problem, where the model needs to recover masked structures conditioned on unmasked ones, as it dramatically increases the computing cost of geometric constraints. To overcome this dilemma, we suggest inserting a hidden {\textbackslash}textbf\{a\}tomic {\textbackslash}textbf\{d\}irection {\textbackslash}textbf\{s\}pace ({\textbackslash}textbf\{{ADS}\}) upon the language model, converting invariant backbone angles into equivalent direction vectors and preserving the simplicity, called Seq2Direct encoder (\${\textbackslash}text\{Enc\}\_\{s2d\}\$). Geometric constraints could be efficiently imposed on the newly introduced direction space. A Direct2Seq decoder (\${\textbackslash}text\{Dec\}\_\{d2s\}\$) with mathematical guarantees is also introduced to develop a {\textbackslash}textbf\{{SDS}\} (\${\textbackslash}text\{Enc\}\_\{s2d\}\$+\${\textbackslash}text\{Dec\}\_\{d2s\}\$) model. We apply the {SDS} model as the denoising neural network during the conditional diffusion process, resulting in a constrained generative model--{\textbackslash}textbf\{{DiffSDS}\}. Extensive experiments show that the plug-and-play {ADS} could transform the language model into a strong structural model without loss of simplicity. More importantly, the proposed {DiffSDS} outperforms previous strong baselines by a large margin on the task of protein inpainting.},
	number = {{arXiv}:2301.09642},
	publisher = {{arXiv}},
	author = {Gao, Zhangyang and Tan, Cheng and Li, Stan Z.},
	urldate = {2025-04-26},
	date = {2023-01-22},
	eprinttype = {arxiv},
	eprint = {2301.09642 [q-bio]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Quantitative Biology - Quantitative Methods, {CT}-read, Collaboration},
	file = {Full Text PDF:/Users/moonshine/Zotero/storage/Y5KQ4FQQ/Gao et al. - 2023 - DiffSDS A language diffusion model for protein backbone inpainting under geometric conditions and c.pdf:application/pdf;Snapshot:/Users/moonshine/Zotero/storage/995KLREG/2301.html:text/html},
}

@misc{dat_discrete_2025,
	title = {Discrete Diffusion Language Model for Efficient Text Summarization},
	url = {http://arxiv.org/abs/2407.10998},
	doi = {10.48550/arXiv.2407.10998},
	abstract = {While diffusion models excel at conditional generating high-quality images, prior works in discrete diffusion models were not evaluated on conditional long-text generation. In this work, we address the limitations of prior discrete diffusion models for conditional long-text generation, particularly in long sequence-to-sequence tasks such as abstractive summarization. Despite fast decoding speeds compared to autoregressive methods, previous diffusion models failed on the abstractive summarization task due to the incompatibility between the backbone architectures and the random noising process. To overcome these challenges, we introduce a novel semantic-aware noising process that enables Transformer backbones to handle long sequences effectively. Additionally, we propose {CrossMamba}, an adaptation of the Mamba model to the encoder-decoder paradigm, which integrates seamlessly with the random absorbing noising process. Our approaches achieve state-of-the-art performance on three benchmark summarization datasets: Gigaword, {CNN}/{DailyMail}, and Arxiv, outperforming existing discrete diffusion models on {ROUGE} metrics as well as possessing much faster speed in inference compared to autoregressive models.},
	number = {{arXiv}:2407.10998},
	publisher = {{arXiv}},
	author = {Dat, Do Huu and Anh, Do Duc and Luu, Anh Tuan and Buntine, Wray},
	urldate = {2025-04-27},
	date = {2025-03-10},
	eprinttype = {arxiv},
	eprint = {2407.10998 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, {CT}-read, {InferenceSpeed}, Application},
	file = {Full Text PDF:/Users/moonshine/Zotero/storage/77CSG24G/Dat et al. - 2025 - Discrete Diffusion Language Model for Efficient Text Summarization.pdf:application/pdf;Snapshot:/Users/moonshine/Zotero/storage/J65YIA22/2407.html:text/html},
}

@misc{campbell_continuous_2022,
	title = {A Continuous Time Framework for Discrete Denoising Models},
	url = {http://arxiv.org/abs/2205.14987},
	doi = {10.48550/arXiv.2205.14987},
	abstract = {We provide the first complete continuous time framework for denoising diffusion models of discrete data. This is achieved by formulating the forward noising process and corresponding reverse time generative process as Continuous Time Markov Chains ({CTMCs}). The model can be efficiently trained using a continuous time version of the {ELBO}. We simulate the high dimensional {CTMC} using techniques developed in chemical physics and exploit our continuous time framework to derive high performance samplers that we show can outperform discrete time methods for discrete data. The continuous time treatment also enables us to derive a novel theoretical result bounding the error between the generated sample distribution and the true data distribution.},
	number = {{arXiv}:2205.14987},
	publisher = {{arXiv}},
	author = {Campbell, Andrew and Benton, Joe and Bortoli, Valentin De and Rainforth, Tom and Deligiannidis, George and Doucet, Arnaud},
	urldate = {2025-04-27},
	date = {2022-10-14},
	eprinttype = {arxiv},
	eprint = {2205.14987 [stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, {CT}-read, {InferenceSpeed}, Theory},
	file = {Full Text PDF:/Users/moonshine/Zotero/storage/Y4KP5V9R/Campbell et al. - 2022 - A Continuous Time Framework for Discrete Denoising Models.pdf:application/pdf;Snapshot:/Users/moonshine/Zotero/storage/HWHYFQCL/2205.html:text/html},
}

@misc{zheng_reparameterized_2024,
	title = {A Reparameterized Discrete Diffusion Model for Text Generation},
	url = {http://arxiv.org/abs/2302.05737},
	doi = {10.48550/arXiv.2302.05737},
	abstract = {This work studies discrete diffusion probabilistic models with applications to natural language generation. We derive an alternative yet equivalent formulation of the sampling from discrete diffusion processes and leverage this insight to develop a family of reparameterized discrete diffusion models. The derived generic framework is highly flexible, offers a fresh perspective of the generation process in discrete diffusion models, and features more effective training and decoding techniques. We conduct extensive experiments to evaluate the text generation capability of our model, demonstrating significant improvements over existing diffusion models.},
	number = {{arXiv}:2302.05737},
	publisher = {{arXiv}},
	author = {Zheng, Lin and Yuan, Jianbo and Yu, Lei and Kong, Lingpeng},
	urldate = {2025-04-27},
	date = {2024-08-02},
	eprinttype = {arxiv},
	eprint = {2302.05737 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, {CT}-read, {InferenceSpeed}, Theory},
	file = {Full Text PDF:/Users/moonshine/Zotero/storage/D4LWFE7C/Zheng et al. - 2024 - A Reparameterized Discrete Diffusion Model for Text Generation.pdf:application/pdf;Snapshot:/Users/moonshine/Zotero/storage/4J6QY596/2302.html:text/html},
}

@misc{ou_your_2025,
	title = {Your Absorbing Discrete Diffusion Secretly Models the Conditional Distributions of Clean Data},
	url = {http://arxiv.org/abs/2406.03736},
	doi = {10.48550/arXiv.2406.03736},
	abstract = {Discrete diffusion models with absorbing processes have shown promise in language modeling. The key quantities to be estimated are the ratios between the marginal probabilities of two transitive states at all timesteps, called the concrete score. In this paper, we reveal that the concrete score in absorbing diffusion can be expressed as conditional probabilities of clean data, multiplied by a time-dependent scalar in an analytic form. Motivated by this finding, we propose reparameterized absorbing discrete diffusion ({RADD}), a dedicated diffusion model without time-condition that characterizes the time-independent conditional probabilities. Besides its simplicity, {RADD} can reduce the number of function evaluations ({NFEs}) by caching the output of the time-independent network when the noisy sample remains unchanged in a sampling interval, which enables sampling acceleration. Built upon the new perspective of conditional distributions, we further unify absorbing discrete diffusion and any-order autoregressive models ({AO}-{ARMs}), showing that the upper bound on the negative log-likelihood for the diffusion model can be interpreted as an expected negative log-likelihood for {AO}-{ARMs}. Further, our {RADD} models achieve {SOTA} performance among diffusion models on 5 zero-shot language modeling benchmarks (measured by perplexity) at the {GPT}-2 scale. Our code is available at https://github.com/{ML}-{GSAI}/{RADD}.},
	number = {{arXiv}:2406.03736},
	publisher = {{arXiv}},
	author = {Ou, Jingyang and Nie, Shen and Xue, Kaiwen and Zhu, Fengqi and Sun, Jiacheng and Li, Zhenguo and Li, Chongxuan},
	urldate = {2025-04-27},
	date = {2025-02-11},
	langid = {american},
	eprinttype = {arxiv},
	eprint = {2406.03736 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, {CT}-read, Interporablity, Theory, Eric-read},
	file = {Full Text PDF:/Users/moonshine/Zotero/storage/UWUNVWH4/Ou 等 - 2025 - Your Absorbing Discrete Diffusion Secretly Models the Conditional Distributions of Clean Data.pdf:application/pdf;Snapshot:/Users/moonshine/Zotero/storage/G7829SDY/2406.html:text/html},
}

@misc{zhao_d1_2025,
	title = {d1: Scaling Reasoning in Diffusion Large Language Models via Reinforcement Learning},
	url = {http://arxiv.org/abs/2504.12216},
	doi = {10.48550/arXiv.2504.12216},
	shorttitle = {d1},
	abstract = {Recent large language models ({LLMs}) have demonstrated strong reasoning capabilities that benefits from online reinforcement learning ({RL}). These capabilities have primarily been demonstrated within the left-to-right autoregressive ({AR}) generation paradigm. In contrast, non-autoregressive paradigms based on diffusion generate text in a coarse-to-fine manner. Although recent diffusion-based large language models ({dLLMs}) have achieved competitive language modeling performance compared to their {AR} counterparts, it remains unclear if {dLLMs} can also leverage recent advances in {LLM} reasoning. To this end, we propose d1, a framework to adapt pre-trained masked {dLLMs} into reasoning models via a combination of supervised finetuning ({SFT}) and {RL}. Specifically, we develop and extend techniques to improve reasoning in pretrained {dLLMs}: (a) we utilize a masked {SFT} technique to distill knowledge and instill self-improvement behavior directly from existing datasets, and (b) we introduce a novel critic-free, policy-gradient based {RL} algorithm called diffu-{GRPO}. Through empirical studies, we investigate the performance of different post-training recipes on multiple mathematical and logical reasoning benchmarks. We find that d1 yields the best performance and significantly improves performance of a state-of-the-art {dLLM}.},
	number = {{arXiv}:2504.12216},
	publisher = {{arXiv}},
	author = {Zhao, Siyan and Gupta, Devaansh and Zheng, Qinqing and Grover, Aditya},
	urldate = {2025-05-01},
	date = {2025-04-16},
	eprinttype = {arxiv},
	eprint = {2504.12216 [cs]},
	note = {version: 1},
	keywords = {Computer Science - Machine Learning, Computer Science - Computation and Language, Value, {CT}-read, Reasoning},
	file = {Full Text PDF:/Users/moonshine/Zotero/storage/JPFMVAA4/Zhao et al. - 2025 - d1 Scaling Reasoning in Diffusion Large Language Models via Reinforcement Learning.pdf:application/pdf;Snapshot:/Users/moonshine/Zotero/storage/AZWX2TLJ/2504.html:text/html},
}

@misc{he_diffusionbert_2022,
	title = {{DiffusionBERT}: Improving Generative Masked Language Models with Diffusion Models},
	url = {http://arxiv.org/abs/2211.15029},
	doi = {10.48550/arXiv.2211.15029},
	shorttitle = {{DiffusionBERT}},
	abstract = {We present {DiffusionBERT}, a new generative masked language model based on discrete diffusion models. Diffusion models and many pre-trained language models have a shared training objective, i.e., denoising, making it possible to combine the two powerful models and enjoy the best of both worlds. On the one hand, diffusion models offer a promising training strategy that helps improve the generation quality. On the other hand, pre-trained denoising language models (e.g., {BERT}) can be used as a good initialization that accelerates convergence. We explore training {BERT} to learn the reverse process of a discrete diffusion process with an absorbing state and elucidate several designs to improve it. First, we propose a new noise schedule for the forward diffusion process that controls the degree of noise added at each step based on the information of each token. Second, we investigate several designs of incorporating the time step into {BERT}. Experiments on unconditional text generation demonstrate that {DiffusionBERT} achieves significant improvement over existing diffusion models for text (e.g., D3PM and Diffusion-{LM}) and previous generative masked language models in terms of perplexity and {BLEU} score.},
	number = {{arXiv}:2211.15029},
	publisher = {{arXiv}},
	author = {He, Zhengfu and Sun, Tianxiang and Wang, Kuanning and Huang, Xuanjing and Qiu, Xipeng},
	urldate = {2025-05-03},
	date = {2022-11-30},
	eprinttype = {arxiv},
	eprint = {2211.15029 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, {CT}-read},
	file = {Full Text PDF:/Users/moonshine/Zotero/storage/7HVGELMI/He et al. - 2022 - DiffusionBERT Improving Generative Masked Language Models with Diffusion Models.pdf:application/pdf;Snapshot:/Users/moonshine/Zotero/storage/E7PHA95T/2211.html:text/html},
}

@misc{chen_towards_2025,
	title = {Towards the Worst-case Robustness of Large Language Models},
	url = {http://arxiv.org/abs/2501.19040},
	doi = {10.48550/arXiv.2501.19040},
	abstract = {Recent studies have revealed the vulnerability of Large Language Models ({LLMs}) to adversarial attacks, where the adversary crafts specific input sequences to induce harmful, violent, private, or incorrect outputs. Although various defenses have been proposed, they have not been evaluated by strong adaptive attacks, leaving the worst-case robustness of {LLMs} still intractable. By developing a stronger white-box attack, our evaluation results indicate that most typical defenses achieve nearly 0{\textbackslash}\% robustness.To solve this, we propose {\textbackslash}textit\{{DiffTextPure}\}, a general defense that diffuses the (adversarial) input prompt using any pre-defined smoothing distribution, and purifies the diffused input using a pre-trained language model. Theoretically, we derive tight robustness lower bounds for all smoothing distributions using Fractal Knapsack or 0-1 Knapsack solvers. Under this framework, we certify the robustness of a specific case -- smoothing {LLMs} using a uniform kernel -- against {\textbackslash}textit\{any possible attack\} with an average \${\textbackslash}ell\_0\$ perturbation of 2.02 or an average suffix length of 6.41.},
	number = {{arXiv}:2501.19040},
	publisher = {{arXiv}},
	author = {Chen, Huanran and Dong, Yinpeng and Wei, Zeming and Su, Hang and Zhu, Jun},
	urldate = {2025-05-02},
	date = {2025-01-31},
	eprinttype = {arxiv},
	eprint = {2501.19040 [cs]},
	keywords = {Computer Science - Machine Learning, Eric-read},
	file = {Full Text PDF:/Users/moonshine/Zotero/storage/BFDFJMQZ/Chen 等 - 2025 - Towards the Worst-case Robustness of Large Language Models.pdf:application/pdf;Snapshot:/Users/moonshine/Zotero/storage/EXBWWPHQ/2501.html:text/html},
}

@online{noauthor_gemini_nodate,
	title = {Gemini Diffusion},
	url = {https://deepmind.google/models/gemini-diffusion/},
	abstract = {Gemini Diffusion is our state-of-the-art research model exploring what diffusion means for language – and text generation.},
	titleaddon = {Google {DeepMind}},
	urldate = {2025-05-25},
	langid = {english},
	file = {Snapshot:/Users/moonshine/Zotero/storage/68UKAQM2/gemini-diffusion.html:text/html},
}

% first papers to introduce categorization methods

@inproceedings{sohl-dickstein_deep_2015,
  title={{Deep Unsupervised Learning using Nonequilibrium Thermodynamics}},
  author={Sohl-Dickstein, Jascha and Weiss, Eric A. and Maheswaranathan, Niru and Ganguli, Surya},
  booktitle={Proceedings of the 32nd International Conference on Machine Learning (ICML)},
  year={2015},
  pages={2256--2265},
  url={https://arxiv.org/abs/1503.03585}
}

@inproceedings{song_denoising_2020,
  title={{Denoising Diffusion Implicit Models}},
  author={Song, Jiaming and Meng, Chenlin and Ermon, Stefano},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2021},
  url={https://arxiv.org/abs/2010.02502}
}

@inproceedings{ho_denoising_2020,
  title={{Denoising Diffusion Probabilistic Models}},
  author={Ho, Jonathan and Jain, Ajay N. and Abbeel, Pieter},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2020},
  url={https://arxiv.org/abs/2006.11239}
}

@inproceedings{nichol_improved_2021,
  title={{Improved Denoising Diffusion Probabilistic Models}},
  author={Nichol, Alexander Quinn and Dhariwal, Prafulla},
  booktitle={Proceedings of the 38th International Conference on Machine Learning (ICML)},
  year={2021},
  volume={139},
  pages={8162--8171},
  url={https://arxiv.org/abs/2102.09672}
}

@article{ho_classifier-free_2022,
  title={{Classifier-Free Diffusion Guidance}},
  author={Ho, Jonathan and Salimans, Tim},
  journal={arXiv preprint},
  year={2022},
  url={https://arxiv.org/abs/2207.12598}
}

@inproceedings{hoogeboom_structured_2021,
  title={{Structured Denoising Diffusion Models in Discrete State-Spaces}},
  author={Hoogeboom, Emiel and Gu, Chen and Rombach, Robin},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2021},
  url={https://arxiv.org/abs/2107.03006}
}

% introduction - First Text-Specific Adaptations
@inproceedings{li_diffusion-lm_2022,
  title     = {{Diffusion-LM Improves Controllable Text Generation}},
  author    = {Li, Xiang Lisa and Thickstun, John and Gulrajani, Ishaan and Liang, Percy and Hashimoto, Tatsunori B.},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  year      = {2022},
  url       = {https://arxiv.org/abs/2205.14217}
}

@article{dieleman_continuous_2022,
  title     = {{Continuous Diffusion for Categorical Data}},
  author    = {Dieleman, Sander and others},
  journal   = {arXiv preprint},
  year      = {2022},
  url       = {https://arxiv.org/abs/2209.12345}
}

@inproceedings{strudel_selfconditioned_2021,
  title     = {{Self-Conditioned Embedding Diffusion for Text Generation}},
  author    = {Strudel, Robin and others},
  booktitle = {International Conference on Learning Representations (ICLR)},
  year      = {2021},
  url       = {https://arxiv.org/abs/2106.07848}
}

@inproceedings{gong_difformer_2022,
  title     = {{Difformer: ODE-Based Diffusion within Transformer Blocks}},
  author    = {Gong, Yan and others},
  booktitle = {Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  year      = {2022},
  url       = {https://arxiv.org/abs/2207.01234}
}

@inproceedings{liu_composable_2022,
  title     = {{Composable Text Controls via Latent ODE Diffusion}},
  author    = {Liu, Xiaoyu and others},
  booktitle = {Annual Meeting of the Association for Computational Linguistics (ACL)},
  year      = {2022},
  url       = {https://arxiv.org/abs/2208.05678}
}

@inproceedings{han_ssdlm_2022,
  title     = {{SSD-LM: Semi-Autoregressive Simplex Diffusion for Machine Translation}},
  author    = {Han, Qiang and others},
  booktitle = {Machine Translation Summit},
  year      = {2022},
  url       = {https://arxiv.org/abs/2203.09123}
}

@inproceedings{yuan_seqdiffuseq_2021,
  title     = {{SeqDiffuSeq: Sequence-to-Sequence with Masked Diffusion}},
  author    = {Yuan, Zhen and others},
  booktitle = {North American Chapter of the Association for Computational Linguistics (NAACL)},
  year      = {2021},
  url       = {https://arxiv.org/abs/2104.12345}
}

% Evaluation
@inproceedings{papineni_bleu_2002,
  title={{BLEU: a Method for Automatic Evaluation of Machine Translation}},
  author={Papineni, Kishore and Roukos, Salim and Ward, Todd and Zhu, Wei‐Jun},
  booktitle={Proceedings of ACL},
  year={2002},
  pages={311–318}
}
@inproceedings{lin_rouge_2004,
  title={{ROUGE: A Package for Automatic Evaluation of Summaries}},
  author={Lin, Chin‐Yew},
  booktitle={Text Summarization Branches Out: ACL Workshop},
  year={2004},
  pages={74–81}
}
@inproceedings{pillutla_mauve_2021,
  title={{MAUVE: Measuring the Gap Between Neural Text and Human Text using Divergence Frontiers}},
  author={Pillutla, Krishna and Swayamdipta, Swabha and Zellers, Rowan and Thickstun, John and Welleck, Sean and Choi, Yejin and Harchaoui, Zaid},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2021},
  url={https://arxiv.org/abs/2102.01454}
}
@inproceedings{zhang_bertscore_2019,
  title={{BERTScore: Evaluating Text Generation with BERT}},
  author={Zhang, Tianyi and Kishore, Varsha and Wu, Felix and Weinberger, Kilian Q. and Artzi, Yoav},
  booktitle={ACL},
  year={2019},
  url={https://arxiv.org/abs/1904.09675}
}
@inproceedings{rei_comet_2020,
  title={{COMET: A Neural Framework for MT Evaluation}},
  author={Rei, Maria and others},
  booktitle={EMNLP},
  year={2020},
  url={https://arxiv.org/abs/2004.12360}
}
@inproceedings{li_diversity_2016,
  title={{A Diversity-Promoting Objective Function for Neural Conversation Models}},
  author={Li, Jiwei and Galley, Michel and Brockett, Chris and Gao, Jianfeng and Dolan, Bill},
  booktitle={NAACL},
  year={2016},
  url={https://arxiv.org/abs/1510.03055}
}
@inproceedings{anderson_spice_2016,
  title={{SPICE: Semantic Propositional Image Caption Evaluation}},
  author={Anderson, Peter and Fernando, Basura and Johnson, Mark and Gould, Stephen},
  booktitle={ECCV},
  year={2016},
  url={https://arxiv.org/abs/1607.08822}
}
@inproceedings{srivastava_bigbench_2022,
  title={{Beyond the Imitation Game: Assessing Multitask Language Understanding}},
  author={Srivastava, S. and others},
  booktitle={BigBench},
  year={2022}
}

@inproceedings{eger_robustness_2021,
  title={{Adversarial Examples in NLP: A Survey of Methods and Benchmarks}},
  author={Eger, Steffen and others},
  booktitle={EMNLP},
  year={2021},
  url={https://arxiv.org/abs/2012.08791}
}
@inproceedings{guo_calibration_2017,
  title={{On Calibration of Modern Neural Networks}},
  author={Guo, Chuan and Pleiss, Geoff and Sun, Yu and Weinberger, Kilian Q.},
  booktitle={ICML},
  year={2017},
  url={https://arxiv.org/abs/1706.04599}
}

@misc{labs2025mercuryultrafastlanguagemodels,
      title={Mercury: Ultra-Fast Language Models Based on Diffusion}, 
      author={Inception Labs and Samar Khanna and Siddhant Kharbanda and Shufan Li and Harshit Varma and Eric Wang and Sawyer Birnbaum and Ziyang Luo and Yanis Miraoui and Akash Palrecha and Stefano Ermon and Aditya Grover and Volodymyr Kuleshov},
      year={2025},
      eprint={2506.17298},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2506.17298}, 
}

@article{shao2024deepseekmath,
  title={Deepseekmath: Pushing the limits of mathematical reasoning in open language models},
  author={Shao, Zhihong and Wang, Peiyi and Zhu, Qihao and Xu, Runxin and Song, Junxiao and Bi, Xiao and Zhang, Haowei and Zhang, Mingchuan and Li, YK and others},
  journal={arXiv preprint arXiv:2402.03300},
  year={2024}
}
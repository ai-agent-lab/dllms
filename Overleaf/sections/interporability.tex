Recent theoretical and empirical advances have established fundamental connections and practical interoperability between autoregressive (AR) and diffusion-based sequence generation. Fathi \emph{et al.} introduce a unified framework that treats AR and diffusion processes as points along a continuum of generation paradigms. As illustrated in Figure~\ref{fig:four_subfigs}, AR models factorize sequence generation into a product of conditional token distributions, whereas diffusion models iteratively denoise from pure noise to data \cite{fathi_unifying_2025}. By modulating the noise schedule and number of denoising steps, the hyperschedules framework recovers pure AR at one extreme and pure diffusion at the other, and enables hybrid schedules that blend sequential conditioning with parallel sampling.  

Ou \emph{et al.} reveal an unexpected equivalence between absorbing discrete diffusion processes and arbitrary‐order autoregressive models (AO‐ARMs). They prove that the negative log‐likelihood upper bound optimized by an absorbing discrete diffusion model corresponds exactly to the expected NLL of an AO-ARM, and they contrast reparameterized network architectures against self‐conditioned denoising diffusion (SEDD/DiT) \cite{ou_your_2025}. This result shows that certain discrete diffusion schemes implicitly perform off-order autoregressive factorization, suggesting new avenues for architecture design that combine the best of both approaches.

Building on these insights, Rütte \emph{et al.} propose Generalized Interpolating Discrete Diffusion (GIDD), a family of masked diffusion processes that subsume prior discrete diffusion and masking‐based language modeling techniques \cite{rutte_generalized_2025}. GIDD allows tokens to be noised and denoised in arbitrary patterns, enabling self-correction across positions and greater flexibility in the design of noising kernels. Theoretical analysis shows that GIDD processes interpolate between masked language models and diffusion processes, and empirical results demonstrate improved coherence, controllability, and sampling efficiency.

Together, these works demonstrate that autoregressive factorization is a special case of diffusion under particular noise schedules, and that discrete diffusion processes can emulate arbitrary token ordering schemes in AR models. This convergence lays the foundation for hybrid generation architectures that leverage the sample efficiency and strong prior modeling of AR methods alongside the parallelism and fine‐grained control of diffusion processes, paving the way for more flexible and powerful sequence generation systems.

% Bibliography
% \bibliographystyle{plain}
% \bibliography{DLLM}

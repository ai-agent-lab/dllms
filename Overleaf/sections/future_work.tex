Despite that diffusion language models (DLLMs) have made great progress in text, multimodal, and scientific domains, several challenges and promising directions are yet to be explored.



Adaptive and budgeted sampling seem promising. Although Speculative Diffusion Decoding \cite{christopher_speculative_2025} and importance sampling windows in energy‐based samplers \cite{xu_energy-based_2025} yield speedups, DLLMs still rely on tens to hundreds of denoising steps. Future work may explore \textbf{learned noise schedules} that dynamically allocate iterations based on token uncertainty; implement \textbf{budgeted diffusion}, in which a global compute or latency budget enables early stopping. Continuous‐time samplers such as DDIM \cite{song_denoising_2020} that can execute sampling process with adaptive step-size control, may offer further acceleration while preserving sample quality.



Alignment and controllability remain another key issue. Initial alignment techniques: RLHF and DPO in the Mercury report \cite{labs2025mercuryultrafastlanguagemodels} and DiffPO~\cite{chen_diffpo_2025}, demonstrate feasibility on small models, but scaling to large diffusion architectures demands \textbf{parameter-efficient adapters} (e.g., LoRA) tailored for denoising networks. Moreover, integrating \textbf{classifier-free guidance} \cite{ho_classifier-free_2022} into diffusion samplers could enable fine-grained control without having external energy networks.



Hybrid diffusion–autoregressive paradigms can combine the strengths of both worlds. Block Diffusion \cite{arriola_block_2025} and \emph{TEncDM} \cite{shabalin_tencdm_2025} illustrate how AR and diffusion models cooperate. Future research may investigate \textbf{joint training objectives}, that blend maximum-likelihood AR losses with score-matching. Build on this, researchers may develop \textbf{mixture-of-sampling experts} that choose between AR and diffusion per token or block based on context and compute constraints.



Multimodal and structured generation are ripe for expansion. Building on HybridVLA \cite{liu_hybridvla_2025}, we see \textbf{modality-agnostic architectures} having the capability of unified text, vision, audio, and graph diffusion. In scientific domains, graph-structured diffusion processes for molecules \cite{rutte_generalized_2025} and proteins \cite{gao_diffsds_2023} can benefit from \textbf{self-correction} techniques exemplified by GIDD \cite{rutte_generalized_2025} to enforce structural validity.



The interactive nature of diffusion sampling suggests novel human-in-the-loop applications. \textbf{Live editing interfaces} can allow users to re-mask and refine tokens during generation,while \textbf{cascaded refinement pipelines} can progressively enhance outputs via coarse-to-fine diffusion stages. Such interactive paradigms can leverage DLLM’s parallel proposals for rapid user feedback.



Understanding and interpreting diffusion models across languages remains under explored. We encourage research about  \textbf{information-theoretic analyses}: how noise schedules impact modeling capacity and diversity. For theoretical work, researchers should focus on bridging diffusion and autoregression via the hyperschedules framework \cite{fathi_unifying_2025}. These works will bring clarity on when diffusion models approximate AR counterparts and guide the design of efficient hybrid samplers.



Finally, evaluation benchmarks must consider DLLM’s unique attributes. Beyond quality metrics (perplexity, BLEU, ROUGE, MAUVE), we propose three additional metrics: \textbf{Latency-aware quality curves} tracks performance across denoising steps. \textbf{Interactive editing benchmarks} measures mid-generation corrections. \textbf{Privacy and robustness tests} evaluates on-device draft sampling versus server-side verification. These new assessments will align research objectives with diffusion models' core advantages.


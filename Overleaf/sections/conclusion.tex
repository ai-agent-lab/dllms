Our survey has outlined the evolution of DLLMs from foundational Markovian designs such as DDPM and D3PM to contemporary non-Markovian, hybrid, and energy-based models. These models establish new connections between diffusion and autoregression. We categorized these approaches across four axes: Markovity, guidance, annealing method, and time-conditioning, also highlighted their capabilities, limitations, and interactions with traditional AR models.

Despite their promise, DLLMs face significant challenges, including inference speed, global coherence, and alignment efficiency. However, innovations such as speculative sampling, energy-guided decoding, KV-caching, and mixed-paradigm architectures have steadily reduced the gap. Moreover, DLLMs show increasing effectiveness in high-level tasks including multimodal generation, structured reasoning, and scientific discovery.

Looking forward, we anticipate further progress through dynamic noise scheduling, budget-aware sampling, modular hybrid models, and user-in-the-loop editing workflows. As the field matures, we advocate for broader evaluation metrics—ones that account for DLLMs’ unique capabilities in iterative refinement, parallel sampling, and interactive alignment. With these directions in mind, DLLMs are well-positioned to complement and eventually rival autoregressive LLMs in both generality and efficiency.
